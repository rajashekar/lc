{
  "data": [
    {
      "context_window": 40960,
      "created": 1753435038,
      "description": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support",
      "id": "alibaba/qwen-3-14b",
      "max_tokens": 8192,
      "name": "Qwen3-14B",
      "object": "model",
      "owned_by": "alibaba",
      "pricing": {
        "input": "0.00000008",
        "output": "0.00000024"
      }
    },
    {
      "context_window": 40960,
      "created": 1753435038,
      "description": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support",
      "id": "alibaba/qwen-3-235b",
      "max_tokens": 8192,
      "name": "Qwen3-235B-A22B",
      "object": "model",
      "owned_by": "alibaba",
      "pricing": {
        "input": "0.0000002",
        "output": "0.0000006"
      }
    },
    {
      "context_window": 40960,
      "created": 1753435038,
      "description": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support",
      "id": "alibaba/qwen-3-30b",
      "max_tokens": 8192,
      "name": "Qwen3-30B-A3B",
      "object": "model",
      "owned_by": "alibaba",
      "pricing": {
        "input": "0.0000001",
        "output": "0.0000003"
      }
    },
    {
      "context_window": 40960,
      "created": 1753435038,
      "description": "Qwen3 is the latest generation of large language models in Qwen series, offering a comprehensive suite of dense and mixture-of-experts (MoE) models. Built upon extensive training, Qwen3 delivers groundbreaking advancements in reasoning, instruction-following, agent capabilities, and multilingual support",
      "id": "alibaba/qwen-3-32b",
      "max_tokens": 8192,
      "name": "Qwen 3.32B",
      "object": "model",
      "owned_by": "alibaba",
      "pricing": {
        "input": "0.0000001",
        "output": "0.0000003"
      }
    },
    {
      "context_window": 262144,
      "created": 1753435038,
      "description": "Qwen3-Coder-480B-A35B-Instruct is Qwen's most agentic code model, featuring significant performance on Agentic Coding, Agentic Browser-Use and other foundational coding tasks, achieving results comparable to Claude Sonnet.",
      "id": "alibaba/qwen3-coder",
      "max_tokens": 8192,
      "name": "Qwen3 Coder",
      "object": "model",
      "owned_by": "alibaba",
      "pricing": {
        "input": "0.0000004",
        "output": "0.0000016"
      }
    },
    {
      "context_window": 300000,
      "created": 1753435038,
      "description": "A very low cost multimodal model that is lightning fast for processing image, video, and text inputs.",
      "id": "amazon/nova-lite",
      "max_tokens": 4096,
      "name": "Nova Lite",
      "object": "model",
      "owned_by": "amazon",
      "pricing": {
        "input": "0.00000006",
        "output": "0.00000024"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "A text-only model that delivers the lowest latency responses at very low cost.",
      "id": "amazon/nova-micro",
      "max_tokens": 4096,
      "name": "Nova Micro",
      "object": "model",
      "owned_by": "amazon",
      "pricing": {
        "input": "0.000000035",
        "output": "0.00000014"
      }
    },
    {
      "context_window": 300000,
      "created": 1753435038,
      "description": "A highly capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks.",
      "id": "amazon/nova-pro",
      "max_tokens": 4096,
      "name": "Nova Pro",
      "object": "model",
      "owned_by": "amazon",
      "pricing": {
        "input": "0.0000008",
        "output": "0.0000032"
      }
    },
    {
      "context_window": 200000,
      "created": 1753435038,
      "description": "Claude 3 Haiku is Anthropic's fastest model yet, designed for enterprise workloads which often involve longer prompts. Haiku to quickly analyze large volumes of documents, such as quarterly filings, contracts, or legal cases, for half the cost of other models in its performance tier.",
      "id": "anthropic/claude-3-haiku",
      "max_tokens": 1024,
      "name": "Claude 3 Haiku",
      "object": "model",
      "owned_by": "anthropic",
      "pricing": {
        "input": "0.00000025",
        "output": "0.00000125"
      }
    },
    {
      "context_window": 200000,
      "created": 1753435038,
      "description": "Claude 3 Opus is Anthropic's most intelligent model, with best-in-market performance on highly complex tasks. It can navigate open-ended prompts and sight-unseen scenarios with remarkable fluency and human-like understanding. Opus shows us the outer limits of what's possible with generative AI.",
      "id": "anthropic/claude-3-opus",
      "max_tokens": 1024,
      "name": "Claude 3 Opus",
      "object": "model",
      "owned_by": "anthropic",
      "pricing": {
        "input": "0.000015",
        "output": "0.000075"
      }
    },
    {
      "context_window": 200000,
      "created": 1753435038,
      "description": "Claude 3.5 Haiku is the next generation of our fastest model. For a similar speed to Claude 3 Haiku, Claude 3.5 Haiku improves across every skill set and surpasses Claude 3 Opus, the largest model in our previous generation, on many intelligence benchmarks.",
      "id": "anthropic/claude-3.5-haiku",
      "max_tokens": 1024,
      "name": "Claude 3.5 Haiku",
      "object": "model",
      "owned_by": "anthropic",
      "pricing": {
        "input": "0.0000008",
        "output": "0.000004"
      }
    },
    {
      "context_window": 200000,
      "created": 1753435038,
      "description": "Claude 3.5 Sonnet strikes the ideal balance between intelligence and speed—particularly for enterprise workloads. It delivers strong performance at a lower cost compared to its peers, and is engineered for high endurance in large-scale AI deployments.",
      "id": "anthropic/claude-3.5-sonnet",
      "max_tokens": 1024,
      "name": "Claude 3.5 Sonnet",
      "object": "model",
      "owned_by": "anthropic",
      "pricing": {
        "input": "0.000003",
        "output": "0.000015"
      }
    },
    {
      "context_window": 200000,
      "created": 1753435038,
      "description": "Claude 3.7 Sonnet is the first hybrid reasoning model and Anthropic's most intelligent model to date. It delivers state-of-the-art performance for coding, content generation, data analysis, and planning tasks, building upon its predecessor Claude 3.5 Sonnet's capabilities in software engineering and computer use.",
      "id": "anthropic/claude-3.7-sonnet",
      "max_tokens": 1024,
      "name": "Claude 3.7 Sonnet",
      "object": "model",
      "owned_by": "anthropic",
      "pricing": {
        "input": "0.000003",
        "output": "0.000015"
      }
    },
    {
      "context_window": 200000,
      "created": 1753435038,
      "description": "Claude Opus 4 is Anthropic's most powerful model yet and the best coding model in the world, leading on SWE-bench (72.5%) and Terminal-bench (43.2%). It delivers sustained performance on long-running tasks that require focused effort and thousands of steps, with the ability to work continuously for several hours—dramatically outperforming all Sonnet models and significantly expanding what AI agents can accomplish.",
      "id": "anthropic/claude-4-opus",
      "max_tokens": 1024,
      "name": "Claude 4 Opus",
      "object": "model",
      "owned_by": "anthropic",
      "pricing": {
        "input": "0.000015",
        "output": "0.000075"
      }
    },
    {
      "context_window": 200000,
      "created": 1753435038,
      "description": "Claude Sonnet 4 significantly improves on Sonnet 3.7's industry-leading capabilities, excelling in coding with a state-of-the-art 72.7% on SWE-bench. The model balances performance and efficiency for internal and external use cases, with enhanced steerability for greater control over implementations. While not matching Opus 4 in most domains, it delivers an optimal mix of capability and practicality.",
      "id": "anthropic/claude-4-sonnet",
      "max_tokens": 1024,
      "name": "Claude 4 Sonnet",
      "object": "model",
      "owned_by": "anthropic",
      "pricing": {
        "input": "0.000003",
        "output": "0.000015"
      }
    },
    {
      "context_window": 256000,
      "created": 1753435038,
      "description": "Command A is Cohere's most performant model to date, excelling at tool use, agents, retrieval augmented generation (RAG), and multilingual use cases. Command A has a context length of 256K, only requires two GPUs to run, and has 150% higher throughput compared to Command R+ 08-2024.",
      "id": "cohere/command-a",
      "max_tokens": 4000,
      "name": "Command A",
      "object": "model",
      "owned_by": "cohere",
      "pricing": {
        "input": "0.0000025",
        "output": "0.00001"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "Command R is a large language model optimized for conversational interaction and long context tasks. It targets the \"scalable\" category of models that balance high performance with strong accuracy, enabling companies to move beyond proof of concept and into production.",
      "id": "cohere/command-r",
      "max_tokens": 1024,
      "name": "Command R",
      "object": "model",
      "owned_by": "cohere",
      "pricing": {
        "input": "0.00000015",
        "output": "0.0000006"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "Command R+ is Cohere's newest large language model, optimized for conversational interaction and long-context tasks. It aims at being extremely performant, enabling companies to move beyond proof of concept and into production.",
      "id": "cohere/command-r-plus",
      "max_tokens": 1024,
      "name": "Command R+",
      "object": "model",
      "owned_by": "cohere",
      "pricing": {
        "input": "0.0000025",
        "output": "0.00001"
      }
    },
    {
      "context_window": 64000,
      "created": 1753435038,
      "description": "The DeepSeek R1 model has undergone a minor version upgrade, with the current version being DeepSeek-R1-0528. In the latest update, DeepSeek R1 has significantly improved its depth of reasoning and inference capabilities by leveraging increased computational resources and introducing algorithmic optimization mechanisms during post-training. The model has demonstrated outstanding performance across various benchmark evaluations, including mathematics, programming, and general logic. Its overall performance is now approaching that of leading models, such as O3 and Gemini 2.5 Pro.",
      "id": "deepseek/deepseek-r1",
      "max_tokens": 4096,
      "name": "DeepSeek-R1",
      "object": "model",
      "owned_by": "deepseek",
      "pricing": {
        "input": "0.00000055",
        "output": "0.00000219"
      }
    },
    {
      "context_window": 131072,
      "created": 1753435038,
      "description": "DeepSeek-R1-Distill-Llama-70B is a distilled, more efficient variant of the 70B Llama model. It preserves strong performance across text-generation tasks, reducing computational overhead for easier deployment and research. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",
      "id": "deepseek/deepseek-r1-distill-llama-70b",
      "max_tokens": 4096,
      "name": "DeepSeek R1 Distill Llama 70B",
      "object": "model",
      "owned_by": "deepseek",
      "pricing": {
        "input": "0.00000075",
        "output": "0.00000099"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "DeepSeek-V3 is an open-source large language model that builds upon LLaMA (Meta's foundational language model) to enable versatile functionalities such as text generation, code completion, and more, served by Fireworks AI.",
      "id": "deepseek/deepseek-v3",
      "max_tokens": 1024,
      "name": "DeepSeek-V3",
      "object": "model",
      "owned_by": "deepseek",
      "pricing": {
        "input": "0.0000009",
        "output": "0.0000009"
      }
    },
    {
      "context_window": 1048576,
      "created": 1753435038,
      "description": "Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, built-in tool use, multimodal generation, and a 1M token context window.",
      "id": "google/gemini-2.0-flash",
      "max_tokens": 4096,
      "name": "Gemini 2.0 Flash",
      "object": "model",
      "owned_by": "google",
      "pricing": {
        "input": "0.00000015",
        "output": "0.0000006"
      }
    },
    {
      "context_window": 1048576,
      "created": 1753435038,
      "description": "Gemini 2.0 Flash delivers next-gen features and improved capabilities, including superior speed, built-in tool use, multimodal generation, and a 1M token context window.",
      "id": "google/gemini-2.0-flash-lite",
      "max_tokens": 4096,
      "name": "Gemini 2.0 Flash Lite",
      "object": "model",
      "owned_by": "google",
      "pricing": {
        "input": "0.000000075",
        "output": "0.0000003"
      }
    },
    {
      "context_window": 1000000,
      "created": 1753435038,
      "description": "Gemini 2.5 Flash is a thinking model that offers great, well-rounded capabilities. It is designed to offer a balance between price and performance with multimodal support and a 1M token context window.",
      "id": "google/gemini-2.5-flash",
      "max_tokens": 4096,
      "name": "Gemini 2.5 Flash",
      "object": "model",
      "owned_by": "google",
      "pricing": {
        "input": "0.0000003",
        "output": "0.0000025"
      }
    },
    {
      "context_window": 1048576,
      "created": 1753435038,
      "description": "Gemini 2.5 Pro is our most advanced reasoning Gemini model, capable of solving complex problems. It features a 2M token context window and supports multimodal inputs including text, images, audio, video, and PDF documents.",
      "id": "google/gemini-2.5-pro",
      "max_tokens": 4096,
      "name": "Gemini 2.5 Pro",
      "object": "model",
      "owned_by": "google",
      "pricing": {
        "input": "0.0000025",
        "output": "0.00001"
      }
    },
    {
      "context_window": 8192,
      "created": 1753435038,
      "description": "9 billion parameter open source model by Google fine-tuned for chat purposes. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",
      "id": "google/gemma-2-9b",
      "max_tokens": 1000,
      "name": "Gemma 2 9B IT",
      "object": "model",
      "owned_by": "google",
      "pricing": {
        "input": "0.0000002",
        "output": "0.0000002"
      }
    },
    {
      "context_window": 32000,
      "created": 1753435038,
      "description": "Mercury Coder Small is ideal for code generation, debugging, and refactoring tasks with minimal latency.",
      "id": "inception/mercury-coder-small",
      "max_tokens": 8192,
      "name": "Mercury Coder Small Beta",
      "object": "model",
      "owned_by": "inception",
      "pricing": {
        "input": "0.00000025",
        "output": "0.000001"
      }
    },
    {
      "context_window": 8192,
      "created": 1753435038,
      "description": "Llama is a 70 billion parameter open source model by Meta fine-tuned for instruction following purposes. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",
      "id": "meta/llama-3-70b",
      "max_tokens": 1000,
      "name": "Llama 3 70B Instruct",
      "object": "model",
      "owned_by": "meta",
      "pricing": {
        "input": "0.00000059",
        "output": "0.00000079"
      }
    },
    {
      "context_window": 8192,
      "created": 1753435038,
      "description": "Llama is a 8 billion parameter open source model by Meta fine-tuned for instruction following purposes. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",
      "id": "meta/llama-3-8b",
      "max_tokens": 1000,
      "name": "Llama 3 8B Instruct",
      "object": "model",
      "owned_by": "meta",
      "pricing": {
        "input": "0.00000005",
        "output": "0.00000008"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "An update to Meta Llama 3 70B Instruct that includes an expanded 128K context length, multilinguality and improved reasoning capabilities.",
      "id": "meta/llama-3.1-70b",
      "max_tokens": 4096,
      "name": "Llama 3.1 70B Instruct",
      "object": "model",
      "owned_by": "meta",
      "pricing": {
        "input": "0.00000072",
        "output": "0.00000072"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "Llama 3.1 8B with 128K context window support, making it ideal for real-time conversational interfaces and data analysis while offering significant cost savings compared to larger models. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",
      "id": "meta/llama-3.1-8b",
      "max_tokens": 4000,
      "name": "Llama 3.1 8B Instruct",
      "object": "model",
      "owned_by": "meta",
      "pricing": {
        "input": "0.00000005",
        "output": "0.00000008"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "Instruction-tuned image reasoning generative model (text + images in / text out) optimized for visual recognition, image reasoning, captioning and answering general questions about the image.",
      "id": "meta/llama-3.2-11b",
      "max_tokens": 4096,
      "name": "Llama 3.2 11B Vision Instruct",
      "object": "model",
      "owned_by": "meta",
      "pricing": {
        "input": "0.00000016",
        "output": "0.00000016"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "Text-only model, supporting on-device use cases such as multilingual local knowledge retrieval, summarization, and rewriting.",
      "id": "meta/llama-3.2-1b",
      "max_tokens": 4096,
      "name": "Llama 3.2 1B Instruct",
      "object": "model",
      "owned_by": "meta",
      "pricing": {
        "input": "0.0000001",
        "output": "0.0000001"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "Text-only model, fine-tuned for supporting on-device use cases such as multilingual local knowledge retrieval, summarization, and rewriting.",
      "id": "meta/llama-3.2-3b",
      "max_tokens": 4096,
      "name": "Llama 3.2 3B Instruct",
      "object": "model",
      "owned_by": "meta",
      "pricing": {
        "input": "0.00000015",
        "output": "0.00000015"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "Instruction-tuned image reasoning generative model (text + images in / text out) optimized for visual recognition, image reasoning, captioning and answering general questions about the image.",
      "id": "meta/llama-3.2-90b",
      "max_tokens": 4096,
      "name": "Llama 3.2 90B Vision Instruct",
      "object": "model",
      "owned_by": "meta",
      "pricing": {
        "input": "0.00000072",
        "output": "0.00000072"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "Where performance meets efficiency. This model supports high-performance conversational AI designed for content creation, enterprise applications, and research, offering advanced language understanding capabilities, including text summarization, classification, sentiment analysis, and code generation.",
      "id": "meta/llama-3.3-70b",
      "max_tokens": 4096,
      "name": "Llama 3.3 70B Instruct",
      "object": "model",
      "owned_by": "meta",
      "pricing": {
        "input": "0.00000072",
        "output": "0.00000072"
      }
    },
    {
      "context_window": 131072,
      "created": 1753435038,
      "description": "The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. Llama 4 Maverick, a 17 billion parameter model with 128 experts. Served by DeepInfra.",
      "id": "meta/llama-4-maverick",
      "max_tokens": 4096,
      "name": "Llama 4 Maverick 17B Instruct",
      "object": "model",
      "owned_by": "meta",
      "pricing": {
        "input": "0.0000002",
        "output": "0.0000006"
      }
    },
    {
      "context_window": 131072,
      "created": 1753435038,
      "description": "The Llama 4 collection of models are natively multimodal AI models that enable text and multimodal experiences. These models leverage a mixture-of-experts architecture to offer industry-leading performance in text and image understanding. Llama 4 Scout, a 17 billion parameter model with 16 experts. Served by DeepInfra.",
      "id": "meta/llama-4-scout",
      "max_tokens": 4096,
      "name": "Llama 4 Scout 17B Instruct",
      "object": "model",
      "owned_by": "meta",
      "pricing": {
        "input": "0.0000001",
        "output": "0.0000003"
      }
    },
    {
      "context_window": 256000,
      "created": 1753435038,
      "description": "Mistral Codestral 25.01 is a state-of-the-art coding model optimized for low-latency, high-frequency use cases. Proficient in over 80 programming languages, it excels at tasks like fill-in-the-middle (FIM), code correction, and test generation.",
      "id": "mistral/codestral",
      "max_tokens": 1024,
      "name": "Mistral Codestral 25.01",
      "object": "model",
      "owned_by": "mistral",
      "pricing": {
        "input": "0.0000003",
        "output": "0.0000009"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "Devstral is an agentic LLM for software engineering tasks, making it a great choice for software engineering agents.",
      "id": "mistral/devstral-small",
      "max_tokens": 8192,
      "name": "Devstral Small",
      "object": "model",
      "owned_by": "mistral",
      "pricing": {
        "input": "0.00000007",
        "output": "0.00000028"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "Complex thinking, backed by deep understanding, with transparent reasoning you can follow and verify. The model excels in maintaining high-fidelity reasoning across numerous languages, even when switching between languages mid-task.",
      "id": "mistral/magistral-medium",
      "max_tokens": 32768,
      "name": "Magistral Medium 2506",
      "object": "model",
      "owned_by": "mistral",
      "pricing": {
        "input": "0.000002",
        "output": "0.000005"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "Complex thinking, backed by deep understanding, with transparent reasoning you can follow and verify. The model excels in maintaining high-fidelity reasoning across numerous languages, even when switching between languages mid-task.",
      "id": "mistral/magistral-small",
      "max_tokens": 32768,
      "name": "Magistral Small 2506",
      "object": "model",
      "owned_by": "mistral",
      "pricing": {
        "input": "0.0000005",
        "output": "0.0000015"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "A compact, efficient model for on-device tasks like smart assistants and local analytics, offering low-latency performance.",
      "id": "mistral/ministral-3b",
      "max_tokens": 1024,
      "name": "Ministral 3B",
      "object": "model",
      "owned_by": "mistral",
      "pricing": {
        "input": "0.00000004",
        "output": "0.00000004"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "A more powerful model with faster, memory-efficient inference, ideal for complex workflows and demanding edge applications.",
      "id": "mistral/ministral-8b",
      "max_tokens": 1024,
      "name": "Ministral 8B",
      "object": "model",
      "owned_by": "mistral",
      "pricing": {
        "input": "0.0000001",
        "output": "0.0000001"
      }
    },
    {
      "context_window": 32000,
      "created": 1753435038,
      "description": "Mistral Large is ideal for complex tasks that require large reasoning capabilities or are highly specialized - like Synthetic Text Generation, Code Generation, RAG, or Agents.",
      "id": "mistral/mistral-large",
      "max_tokens": 1024,
      "name": "Mistral Large",
      "object": "model",
      "owned_by": "mistral",
      "pricing": {
        "input": "0.000002",
        "output": "0.000006"
      }
    },
    {
      "context_window": 32768,
      "created": 1753435038,
      "description": "Mistral Saba 24B is a 24 billion parameter open source model by Mistral.ai. Saba is a specialized model trained to excel in Arabic, Farsi, Urdu, Hebrew, and Indic languages. Served by Groq with their custom Language Processing Units (LPUs) hardware to provide fast and efficient inference.",
      "id": "mistral/mistral-saba-24b",
      "max_tokens": 16384,
      "name": "Mistral Saba 24B",
      "object": "model",
      "owned_by": "mistral",
      "pricing": {
        "input": "0.00000079",
        "output": "0.00000079"
      }
    },
    {
      "context_window": 32000,
      "created": 1753435038,
      "description": "Mistral Small is the ideal choice for simple tasks that one can do in bulk - like Classification, Customer Support, or Text Generation. It offers excellent performance at an affordable price point.",
      "id": "mistral/mistral-small",
      "max_tokens": 1024,
      "name": "Mistral Small",
      "object": "model",
      "owned_by": "mistral",
      "pricing": {
        "input": "0.0000001",
        "output": "0.0000003"
      }
    },
    {
      "context_window": 2048,
      "created": 1753435038,
      "description": "8x22b Instruct model. 8x22b is mixture-of-experts open source model by Mistral served by Fireworks.",
      "id": "mistral/mixtral-8x22b-instruct",
      "max_tokens": 256,
      "name": "Mixtral MoE 8x22B Instruct",
      "object": "model",
      "owned_by": "mistral",
      "pricing": {
        "input": "0.0000012",
        "output": "0.0000012"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "A 12B model with image understanding capabilities in addition to text.",
      "id": "mistral/pixtral-12b",
      "max_tokens": 1024,
      "name": "Pixtral 12B 2409",
      "object": "model",
      "owned_by": "mistral",
      "pricing": {
        "input": "0.00000015",
        "output": "0.00000015"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "Pixtral Large is the second model in our multimodal family and demonstrates frontier-level image understanding. Particularly, the model is able to understand documents, charts and natural images, while maintaining the leading text-only understanding of Mistral Large 2.",
      "id": "mistral/pixtral-large",
      "max_tokens": 1024,
      "name": "Pixtral Large",
      "object": "model",
      "owned_by": "mistral",
      "pricing": {
        "input": "0.000002",
        "output": "0.000006"
      }
    },
    {
      "context_window": 131072,
      "created": 1753435038,
      "description": "Kimi K2 is a large-scale Mixture-of-Experts (MoE) language model developed by Moonshot AI, featuring 1 trillion total parameters with 32 billion active per forward pass. It is optimized for agentic capabilities, including advanced tool use, reasoning, and code synthesis. Kimi K2 excels across a broad range of benchmarks, particularly in coding (LiveCodeBench, SWE-bench), reasoning (ZebraLogic, GPQA), and tool-use (Tau2, AceBench) tasks.",
      "id": "moonshotai/kimi-k2",
      "max_tokens": 8192,
      "name": "Kimi K2",
      "object": "model",
      "owned_by": "moonshotai",
      "pricing": {
        "input": "0.00000055",
        "output": "0.0000022"
      }
    },
    {
      "context_window": 32768,
      "created": 1753435038,
      "description": "Morph offers a specialized AI model that applies code changes suggested by frontier models (like Claude or GPT-4o) to your existing code files FAST - 4500+ tokens/second. It acts as the final step in the AI coding workflow. Supports 16k input tokens and 16k output tokens.",
      "id": "morph/morph-v3-fast",
      "max_tokens": 4096,
      "name": "Morph V3 Fast",
      "object": "model",
      "owned_by": "morph",
      "pricing": {
        "input": "0.0000008",
        "output": "0.0000012"
      }
    },
    {
      "context_window": 32768,
      "created": 1753435038,
      "description": "Morph offers a specialized AI model that applies code changes suggested by frontier models (like Claude or GPT-4o) to your existing code files FAST - 2500+ tokens/second. It acts as the final step in the AI coding workflow. Supports 16k input tokens and 16k output tokens.",
      "id": "morph/morph-v3-large",
      "max_tokens": 4096,
      "name": "Morph V3 Large",
      "object": "model",
      "owned_by": "morph",
      "pricing": {
        "input": "0.0000009",
        "output": "0.0000019"
      }
    },
    {
      "context_window": 4096,
      "created": 1753435038,
      "description": "OpenAI's most capable and cost effective model in the GPT-3.5 family optimized for chat purposes, but also works well for traditional completions tasks.",
      "id": "openai/gpt-3.5-turbo",
      "max_tokens": 500,
      "name": "GPT-3.5 Turbo",
      "object": "model",
      "owned_by": "openai",
      "pricing": {
        "input": "0.0000005",
        "output": "0.0000015"
      }
    },
    {
      "context_window": 4096,
      "created": 1753435038,
      "description": "Similar capabilities as GPT-3 era models. Compatible with legacy Completions endpoint and not Chat Completions.",
      "id": "openai/gpt-3.5-turbo-instruct",
      "max_tokens": 200,
      "name": "GPT-3.5 Turbo Instruct",
      "object": "model",
      "owned_by": "openai",
      "pricing": {
        "input": "0.0000015",
        "output": "0.000002"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "gpt-4-turbo from OpenAI has broad general knowledge and domain expertise allowing it to follow complex instructions in natural language and solve difficult problems accurately. It has a knowledge cutoff of April 2023 and a 128,000 token context window.",
      "id": "openai/gpt-4-turbo",
      "max_tokens": 1024,
      "name": "GPT-4 Turbo",
      "object": "model",
      "owned_by": "openai",
      "pricing": {
        "input": "0.00001",
        "output": "0.00003"
      }
    },
    {
      "context_window": 1047576,
      "created": 1753435038,
      "description": "GPT 4.1 is OpenAI's flagship model for complex tasks. It is well suited for problem solving across domains.",
      "id": "openai/gpt-4.1",
      "max_tokens": 8192,
      "name": "GPT-4.1",
      "object": "model",
      "owned_by": "openai",
      "pricing": {
        "input": "0.000002",
        "output": "0.000008"
      }
    },
    {
      "context_window": 1047576,
      "created": 1753435038,
      "description": "GPT 4.1 mini provides a balance between intelligence, speed, and cost that makes it an attractive model for many use cases.",
      "id": "openai/gpt-4.1-mini",
      "max_tokens": 8192,
      "name": "GPT-4.1 mini",
      "object": "model",
      "owned_by": "openai",
      "pricing": {
        "input": "0.0000004",
        "output": "0.0000016"
      }
    },
    {
      "context_window": 1047576,
      "created": 1753435038,
      "description": "GPT-4.1 nano is the fastest, most cost-effective GPT 4.1 model.",
      "id": "openai/gpt-4.1-nano",
      "max_tokens": 8192,
      "name": "GPT-4.1 nano",
      "object": "model",
      "owned_by": "openai",
      "pricing": {
        "input": "0.0000001",
        "output": "0.0000004"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "GPT-4o from OpenAI has broad general knowledge and domain expertise allowing it to follow complex instructions in natural language and solve difficult problems accurately. It matches GPT-4 Turbo performance with a faster and cheaper API.",
      "id": "openai/gpt-4o",
      "max_tokens": 1024,
      "name": "GPT-4o",
      "object": "model",
      "owned_by": "openai",
      "pricing": {
        "input": "0.0000025",
        "output": "0.00001"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "GPT-4o mini from OpenAI is their most advanced and cost-efficient small model. It is multi-modal (accepting text or image inputs and outputting text) and has higher intelligence than gpt-3.5-turbo but is just as fast.",
      "id": "openai/gpt-4o-mini",
      "max_tokens": 1024,
      "name": "GPT-4o mini",
      "object": "model",
      "owned_by": "openai",
      "pricing": {
        "input": "0.00000015",
        "output": "0.0000006"
      }
    },
    {
      "context_window": 200000,
      "created": 1753435038,
      "description": "o1 is OpenAI's flagship reasoning model, designed for complex problems that require deep thinking. It provides strong reasoning capabilities with improved accuracy for complex multi-step tasks.",
      "id": "openai/o1",
      "max_tokens": 4096,
      "name": "o1",
      "object": "model",
      "owned_by": "openai",
      "pricing": {
        "input": "0.000015",
        "output": "0.00006"
      }
    },
    {
      "context_window": 200000,
      "created": 1753435038,
      "description": "OpenAI's o3 is their most powerful reasoning model, setting new state-of-the-art benchmarks in coding, math, science, and visual perception. It excels at complex queries requiring multi-faceted analysis, with particular strength in analyzing images, charts, and graphics.",
      "id": "openai/o3",
      "max_tokens": 16384,
      "name": "o3",
      "object": "model",
      "owned_by": "openai",
      "pricing": {
        "input": "0.000002",
        "output": "0.000008"
      }
    },
    {
      "context_window": 200000,
      "created": 1753435038,
      "description": "o3-mini is OpenAI's most recent small reasoning model, providing high intelligence at the same cost and latency targets of o1-mini.",
      "id": "openai/o3-mini",
      "max_tokens": 4096,
      "name": "o3-mini",
      "object": "model",
      "owned_by": "openai",
      "pricing": {
        "input": "0.0000011",
        "output": "0.0000044"
      }
    },
    {
      "context_window": 200000,
      "created": 1753435038,
      "description": "OpenAI's o4-mini delivers fast, cost-efficient reasoning with exceptional performance for its size, particularly excelling in math (best-performing on AIME benchmarks), coding, and visual tasks.",
      "id": "openai/o4-mini",
      "max_tokens": 16384,
      "name": "o4-mini",
      "object": "model",
      "owned_by": "openai",
      "pricing": {
        "input": "0.0000011",
        "output": "0.0000044"
      }
    },
    {
      "context_window": 127000,
      "created": 1753435038,
      "description": "Perplexity's lightweight offering with search grounding, quicker and cheaper than Sonar Pro.",
      "id": "perplexity/sonar",
      "max_tokens": 1024,
      "name": "Sonar",
      "object": "model",
      "owned_by": "perplexity",
      "pricing": {
        "input": "0.000001",
        "output": "0.000001"
      }
    },
    {
      "context_window": 200000,
      "created": 1753435038,
      "description": "Perplexity's premier offering with search grounding, supporting advanced queries and follow-ups.",
      "id": "perplexity/sonar-pro",
      "max_tokens": 1024,
      "name": "Sonar Pro",
      "object": "model",
      "owned_by": "perplexity",
      "pricing": {
        "input": "0.000003",
        "output": "0.000015"
      }
    },
    {
      "context_window": 127000,
      "created": 1753435038,
      "description": "A reasoning-focused model that outputs Chain of Thought (CoT) in responses, providing detailed explanations with search grounding.",
      "id": "perplexity/sonar-reasoning",
      "max_tokens": 1024,
      "name": "Sonar Reasoning",
      "object": "model",
      "owned_by": "perplexity",
      "pricing": {
        "input": "0.000001",
        "output": "0.000005"
      }
    },
    {
      "context_window": 127000,
      "created": 1753435038,
      "description": "A premium reasoning-focused model that outputs Chain of Thought (CoT) in responses, providing comprehensive explanations with enhanced search capabilities and multiple search queries per request.",
      "id": "perplexity/sonar-reasoning-pro",
      "max_tokens": 1024,
      "name": "Sonar Reasoning Pro",
      "object": "model",
      "owned_by": "perplexity",
      "pricing": {
        "input": "0.000002",
        "output": "0.000008"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "Access the model behind v0 to generate, fix, and optimize modern web apps with framework-specific reasoning and up-to-date knowledge.",
      "id": "vercel/v0-1.0-md",
      "max_tokens": 1024,
      "name": "v0-1.0-md",
      "object": "model",
      "owned_by": "vercel",
      "pricing": {
        "input": "0.000003",
        "output": "0.000015"
      }
    },
    {
      "context_window": 128000,
      "created": 1753435038,
      "description": "Access the model behind v0 to generate, fix, and optimize modern web apps with framework-specific reasoning and up-to-date knowledge.",
      "id": "vercel/v0-1.5-md",
      "max_tokens": 8192,
      "name": "v0-1.5-md",
      "object": "model",
      "owned_by": "vercel",
      "pricing": {
        "input": "0.000003",
        "output": "0.000015"
      }
    },
    {
      "context_window": 131072,
      "created": 1753435038,
      "description": "Grok 2 is a frontier language model with state-of-the-art reasoning capabilities. It features advanced capabilities in chat, coding, and reasoning, outperforming both Claude 3.5 Sonnet and GPT-4-Turbo on the LMSYS leaderboard.",
      "id": "xai/grok-2",
      "max_tokens": 1024,
      "name": "Grok 2",
      "object": "model",
      "owned_by": "xai",
      "pricing": {
        "input": "0.000002",
        "output": "0.00001"
      }
    },
    {
      "context_window": 32768,
      "created": 1753435038,
      "description": "Grok 2 vision model excels in vision-based tasks, delivering state-of-the-art performance in visual math reasoning (MathVista) and document-based question answering (DocVQA). It can process a wide variety of visual information including documents, diagrams, charts, screenshots, and photographs.",
      "id": "xai/grok-2-vision",
      "max_tokens": 1024,
      "name": "Grok 2 Vision",
      "object": "model",
      "owned_by": "xai",
      "pricing": {
        "input": "0.000002",
        "output": "0.00001"
      }
    },
    {
      "context_window": 131072,
      "created": 1753435038,
      "description": "xAI's flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science.",
      "id": "xai/grok-3",
      "max_tokens": 8192,
      "name": "Grok 3 Beta",
      "object": "model",
      "owned_by": "xai",
      "pricing": {
        "input": "0.000003",
        "output": "0.000015"
      }
    },
    {
      "context_window": 131072,
      "created": 1753435038,
      "description": "xAI's flagship model that excels at enterprise use cases like data extraction, coding, and text summarization. Possesses deep domain knowledge in finance, healthcare, law, and science. The fast model variant is served on faster infrastructure, offering response times that are significantly faster than the standard. The increased speed comes at a higher cost per output token.",
      "id": "xai/grok-3-fast",
      "max_tokens": 8192,
      "name": "Grok 3 Fast Beta",
      "object": "model",
      "owned_by": "xai",
      "pricing": {
        "input": "0.000005",
        "output": "0.000025"
      }
    },
    {
      "context_window": 131072,
      "created": 1753435038,
      "description": "xAI's lightweight model that thinks before responding. Great for simple or logic-based tasks that do not require deep domain knowledge. The raw thinking traces are accessible.",
      "id": "xai/grok-3-mini",
      "max_tokens": 8192,
      "name": "Grok 3 Mini Beta",
      "object": "model",
      "owned_by": "xai",
      "pricing": {
        "input": "0.0000003",
        "output": "0.0000005"
      }
    },
    {
      "context_window": 131072,
      "created": 1753435038,
      "description": "xAI's lightweight model that thinks before responding. Great for simple or logic-based tasks that do not require deep domain knowledge. The raw thinking traces are accessible. The fast model variant is served on faster infrastructure, offering response times that are significantly faster than the standard. The increased speed comes at a higher cost per output token.",
      "id": "xai/grok-3-mini-fast",
      "max_tokens": 8192,
      "name": "Grok 3 Mini Fast Beta",
      "object": "model",
      "owned_by": "xai",
      "pricing": {
        "input": "0.0000006",
        "output": "0.000004"
      }
    },
    {
      "context_window": 256000,
      "created": 1753435038,
      "description": "xAI's latest and greatest flagship model, offering unparalleled performance in natural language, math and reasoning - the perfect jack of all trades.",
      "id": "xai/grok-4",
      "max_tokens": 8192,
      "name": "Grok 4",
      "object": "model",
      "owned_by": "xai",
      "pricing": {
        "input": "0.000003",
        "output": "0.000015"
      }
    }
  ],
  "object": "list"
}