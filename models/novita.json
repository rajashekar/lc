{
  "data": [
    {
      "context_size": 163840,
      "created": 1742909352,
      "description": "DeepSeek V3, a 685B-parameter, mixture-of-experts model, is the latest iteration of the flagship chat model family from the DeepSeek team.",
      "display_name": "DeepSeek V3 0324",
      "endpoints": [
        "completion"
      ],
      "features": [
        "function-calling"
      ],
      "id": "deepseek/deepseek-v3-0324",
      "input_token_price_per_m": 2800,
      "max_output_tokens": 163840,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 11400,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "deepseek/deepseek-v3-0324"
    },
    {
      "context_size": 131072,
      "created": 1752263515,
      "description": "Kimi K2 is a state-of-the-art mixture-of-experts (MoE) language model with 32 billion activated parameters and 1 trillion total parameters. Trained with the Muon optimizer, Kimi K2 achieves exceptional performance across frontier knowledge, reasoning, and coding tasks while being meticulously optimized for agentic capabilities.Specifically designed for tool use, reasoning, and autonomous problem-solving.",
      "display_name": "Kimi K2 Instruct",
      "features": [
        "function-calling",
        "structured-outputs"
      ],
      "id": "moonshotai/kimi-k2-instruct",
      "input_token_price_per_m": 5700,
      "max_output_tokens": 131072,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 23000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "moonshotai/kimi-k2-instruct"
    },
    {
      "context_size": 262144,
      "created": 1753233789,
      "description": "Qwen3-Coder-480B-A35B-Instruct is a cutting-edge open coding model from Qwen, matching Claude Sonnet’s performance in agentic programming, browser automation, and core development tasks. With native 256K context (extendable to 1M tokens via YaRN), it excels at repository-scale analysis and features specialized function-call support for platforms like Qwen Code and CLINE—making it ideal for complex, real-world development workflows.",
      "display_name": "Qwen3 Coder 480B A35B Instruct",
      "endpoints": [
        "completion"
      ],
      "features": [
        "function-calling",
        "structured-outputs"
      ],
      "id": "qwen/qwen3-coder-480b-a35b-instruct",
      "input_token_price_per_m": 20000,
      "max_output_tokens": 65536,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 20000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "qwen/qwen3-coder-480b-a35b-instruct"
    },
    {
      "context_size": 163840,
      "created": 1748457624,
      "description": "DeepSeek R1 0528 is the latest open-source model released by the DeepSeek team, featuring impressive reasoning capabilities, particularly achieving performance comparable to OpenAI's o1 model in mathematics, coding, and reasoning tasks.",
      "display_name": "DeepSeek R1 0528",
      "endpoints": [
        "completion"
      ],
      "features": [
        "function-calling",
        "structured-outputs"
      ],
      "id": "deepseek/deepseek-r1-0528",
      "input_token_price_per_m": 7000,
      "max_output_tokens": 163840,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 25000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "deepseek/deepseek-r1-0528"
    },
    {
      "context_size": 262144,
      "created": 1753176794,
      "description": "Qwen3-235B-A22B-Instruct-2507 is a multilingual, instruction-tuned mixture-of-experts language model based on the Qwen3-235B architecture, with 22B active parameters per forward pass. It is optimized for general-purpose text generation, including instruction following, logical reasoning, math, code, and tool usage. The model supports a native 262K context length and does not implement \"thinking mode\" (<think> blocks).\nCompared to its base variant, this version delivers significant gains in knowledge coverage, long-context reasoning, coding benchmarks, and alignment with open-ended tasks. It is particularly strong on multilingual understanding, math reasoning (e.g., AIME, HMMT), and alignment evaluations like Arena-Hard and WritingBench.",
      "display_name": "Qwen3 235B A22B Instruct 2507",
      "endpoints": [
        "completion"
      ],
      "features": [
        "structured-outputs",
        "function-calling"
      ],
      "id": "qwen/qwen3-235b-a22b-instruct-2507",
      "input_token_price_per_m": 1500,
      "max_output_tokens": 262144,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 8000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "qwen/qwen3-235b-a22b-instruct-2507"
    },
    {
      "context_size": 123000,
      "created": 1751258620,
      "description": "The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio",
      "display_name": "ERNIE 4.5 VL 424B A47B",
      "endpoints": [
        "completion"
      ],
      "features": [
        "function-calling",
        "vision"
      ],
      "id": "baidu/ernie-4.5-vl-424b-a47b",
      "input_token_price_per_m": 4200,
      "max_output_tokens": 16000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 12500,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "baidu/ernie-4.5-vl-424b-a47b"
    },
    {
      "context_size": 123000,
      "created": 1751243379,
      "description": "The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio",
      "display_name": "ERNIE 4.5 300B A47B",
      "endpoints": [
        "completion"
      ],
      "features": [
        "function-calling",
        "structured-outputs"
      ],
      "id": "baidu/ernie-4.5-300b-a47b-paddle",
      "input_token_price_per_m": 2800,
      "max_output_tokens": 12000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 11000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "baidu/ernie-4.5-300b-a47b-paddle"
    },
    {
      "context_size": 40960,
      "created": 1745897181,
      "description": "Achieves effective integration of inference and non-inference modes, allowing seamless switching between modes during conversations. Its inference capability matches that of QwQ-32B with a smaller parameter size, and its general capabilities significantly surpass those of Qwen2.5-14B, reaching the state-of-the-art (SOTA) level among models of the same scale.",
      "display_name": "Qwen3 30B A3B",
      "endpoints": [
        "completion"
      ],
      "id": "qwen/qwen3-30b-a3b-fp8",
      "input_token_price_per_m": 1000,
      "max_output_tokens": 20000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 4500,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "qwen/qwen3-30b-a3b-fp8"
    },
    {
      "context_size": 1000000,
      "created": 1750139830,
      "description": "MiniMax-M1: The World's First Open-Weight, Large-Scale Hybrid Attention Inference Model\n\nMiniMax-M1 adopts a Mixture of Experts (MoE) architecture and integrates the Flash Attention mechanism. The model contains a total of 456 billion parameters, with 45.9 billion parameters activated per token.\n\nNatively, the M1 model supports a context length of 1 million tokens—8 times that of DeepSeek R1. Additionally, by combining the CISPO algorithm with an efficient hybrid attention design for reinforcement learning training, MiniMax-M1 achieves industry-leading performance in long-context reasoning and real-world software engineering scenarios.",
      "display_name": "MiniMax M1",
      "features": [
        "function-calling"
      ],
      "id": "minimaxai/minimax-m1-80k",
      "input_token_price_per_m": 5500,
      "max_output_tokens": 40000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 22000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "minimaxai/minimax-m1-80k"
    },
    {
      "context_size": 128000,
      "created": 1748526709,
      "description": "DeepSeek-R1-0528-Qwen3-8B is a high-performance reasoning model based on the Qwen3 8B Base model, enhanced through the integration of DeepSeek-R1-0528's Chain-of-Thought (CoT) optimization. In the AIME 2024 evaluation, this open-source model achieved state-of-the-art (SOTA) performance, delivering a 10% improvement over the original Qwen3 8B while matching the reasoning capabilities of the much larger 235-billion-parameter Qwen3-235B-thinking. ",
      "display_name": "DeepSeek R1 0528 Qwen3 8B",
      "endpoints": [
        "completion"
      ],
      "id": "deepseek/deepseek-r1-0528-qwen3-8b",
      "input_token_price_per_m": 600,
      "max_output_tokens": 32000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 900,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "deepseek/deepseek-r1-0528-qwen3-8b"
    },
    {
      "context_size": 40960,
      "created": 1745897287,
      "description": "Achieves effective integration of inference and non-inference modes, allowing seamless switching between modes during conversations. Its inference capability matches that of QwQ-32B with a smaller parameter size, and its general capabilities significantly surpass those of Qwen2.5-14B, reaching the state-of-the-art (SOTA) level among models of the same scale.",
      "display_name": "Qwen3 32B",
      "endpoints": [
        "completion"
      ],
      "id": "qwen/qwen3-32b-fp8",
      "input_token_price_per_m": 1000,
      "max_output_tokens": 20000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 4500,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "qwen/qwen3-32b-fp8"
    },
    {
      "context_size": 32768,
      "created": 1742888969,
      "description": "Qwen2.5-VL, the latest vision-language model in the Qwen2.5 series, delivers enhanced multimodal capabilities including advanced visual comprehension for object/text recognition, chart/layout analysis, and agent-based dynamic tool orchestration. It processes long-form videos (>1 hour) with key event detection while enabling precise spatial annotation through bounding boxes or coordinate points. The model specializes in structured data extraction from scanned documents (invoices, tables, etc.) and achieves state-of-the-art performance across multimodal benchmarks encompassing image understanding, temporal video analysis, and agent task evaluations.",
      "display_name": "Qwen2.5 VL 72B Instruct",
      "endpoints": [
        "completion"
      ],
      "features": [
        "vision"
      ],
      "id": "qwen/qwen2.5-vl-72b-instruct",
      "input_token_price_per_m": 8000,
      "max_output_tokens": 32768,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 8000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "qwen/qwen2.5-vl-72b-instruct"
    },
    {
      "context_size": 40960,
      "created": 1745897024,
      "description": "Achieves effective integration of inference and non-inference modes, enabling seamless switching between modes during conversations. The model's inference capability significantly surpasses that of QwQ, and its general capabilities exceed those of Qwen2.5-72B-Instruct, reaching the state-of-the-art (SOTA) level among models of the same scale.",
      "display_name": "Qwen3 235B A22B",
      "endpoints": [
        "completion"
      ],
      "id": "qwen/qwen3-235b-a22b-fp8",
      "input_token_price_per_m": 2000,
      "max_output_tokens": 20000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 8000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "qwen/qwen3-235b-a22b-fp8"
    },
    {
      "context_size": 64000,
      "created": 1741174512,
      "description": "DeepSeek-V3 is the latest model from the DeepSeek team, building upon the instruction following and coding abilities of the previous versions. Pre-trained on nearly 15 trillion tokens, the reported evaluations reveal that the model outperforms other open-source models and rivals leading closed-source models.",
      "display_name": "DeepSeek V3 (Turbo)\t",
      "endpoints": [
        "completion"
      ],
      "features": [
        "function-calling"
      ],
      "id": "deepseek/deepseek-v3-turbo",
      "input_token_price_per_m": 4000,
      "max_output_tokens": 16000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 13000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "deepseek/deepseek-v3-turbo"
    },
    {
      "context_size": 65536,
      "created": 1752056181,
      "description": "GLM-4.1V-9B-Thinking is an open-source Vision-Language Model (VLM) jointly released by Zhipu AI and Tsinghua University’s KEG Lab, specifically designed to handle complex multimodal cognitive tasks. Built upon the GLM-4-9B-0414 base model, it integrates Chain-of-Thought (CoT) reasoning and employs reinforcement learning strategies, significantly enhancing its cross-modal reasoning capabilities and stability. As a lightweight model with 9B parameters, it strikes an optimal balance between deployment efficiency and performance. Across 28 authoritative benchmark evaluations, it matches or surpasses the performance of the 72B-parameter Qwen-2.5-VL-72B in 18 metrics. The model excels in tasks such as image-text understanding, mathematical and scientific reasoning, and video comprehension, while also supporting 4K-resolution images and arbitrary aspect ratios.",
      "display_name": "GLM 4.1V 9B Thinking",
      "features": [
        "vision"
      ],
      "id": "thudm/glm-4.1v-9b-thinking",
      "input_token_price_per_m": 350,
      "max_output_tokens": 8000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 1380,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "thudm/glm-4.1v-9b-thinking"
    },
    {
      "context_size": 1048576,
      "created": 1743906990,
      "description": "Llama 4 Maverick 17B Instruct (128E) is a high-capacity multimodal language model from Meta, built on a mixture-of-experts (MoE) architecture with 128 experts and 17 billion active parameters per forward pass (400B total). It supports multilingual text and image input, and produces multilingual text and code output across 12 supported languages. Optimized for vision-language tasks, Maverick is instruction-tuned for assistant-like behavior, image reasoning, and general-purpose multimodal interaction.\n\nMaverick features early fusion for native multimodality and a 1 million token context window. It was trained on a curated mixture of public, licensed, and Meta-platform data, covering ~22 trillion tokens, with a knowledge cutoff in August 2024. Released on April 5, 2025 under the Llama 4 Community License, Maverick is suited for research and commercial applications requiring advanced multimodal understanding and high model throughput.",
      "display_name": "Llama 4 Maverick Instruct",
      "endpoints": [
        "completion"
      ],
      "features": [
        "function-calling",
        "vision"
      ],
      "id": "meta-llama/llama-4-maverick-17b-128e-instruct-fp8",
      "input_token_price_per_m": 1700,
      "max_output_tokens": 1048576,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 8500,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "meta-llama/llama-4-maverick-17b-128e-instruct-fp8"
    },
    {
      "context_size": 32000,
      "created": 1742889385,
      "description": "Gemma 3 introduces multimodality, supporting vision-language input and text outputs. It handles context windows up to 32k tokens, understands over 140 languages, and offers improved math, reasoning, and chat capabilities, including structured outputs. Gemma 3 27B is Google's latest open source model, successor to Gemma.",
      "display_name": "Gemma 3 27B",
      "endpoints": [
        "completion"
      ],
      "features": [
        "structured-outputs",
        "vision"
      ],
      "id": "google/gemma-3-27b-it",
      "input_token_price_per_m": 1190,
      "max_output_tokens": 32000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 2000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "google/gemma-3-27b-it"
    },
    {
      "context_size": 64000,
      "created": 1741174465,
      "description": "DeepSeek R1 is the latest open-source model released by the DeepSeek team, featuring impressive reasoning capabilities, particularly achieving performance comparable to OpenAI's o1 model in mathematics, coding, and reasoning tasks.",
      "display_name": "DeepSeek R1 (Turbo)\t",
      "endpoints": [
        "completion"
      ],
      "features": [
        "function-calling"
      ],
      "id": "deepseek/deepseek-r1-turbo",
      "input_token_price_per_m": 7000,
      "max_output_tokens": 16000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 25000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "deepseek/deepseek-r1-turbo"
    },
    {
      "context_size": 8192,
      "created": 1732875917,
      "description": "Sao10K/L3-8B-Stheno-v3.2 is a highly skilled actor that excels at fully immersing itself in any role assigned.",
      "display_name": "L3 8B Stheno V3.2",
      "endpoints": [
        "completion"
      ],
      "id": "Sao10K/L3-8B-Stheno-v3.2",
      "input_token_price_per_m": 500,
      "max_output_tokens": 32000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 500,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "Sao10K/L3-8B-Stheno-v3.2"
    },
    {
      "context_size": 4096,
      "created": 1714024873,
      "description": "The idea behind this merge is that each layer is composed of several tensors, which are in turn responsible for specific functions. Using MythoLogic-L2's robust understanding as its input and Huginn's extensive writing capability as its output seems to have resulted in a model that exceeds at both, confirming my theory. (More details to be released at a later time).",
      "display_name": "Mythomax L2 13B",
      "endpoints": [
        "completion"
      ],
      "id": "gryphe/mythomax-l2-13b",
      "input_token_price_per_m": 900,
      "max_output_tokens": 32000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 900,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "gryphe/mythomax-l2-13b"
    },
    {
      "context_size": 160000,
      "created": 1746010994,
      "description": "DeepSeek Launches Open-Source Model DeepSeek-Prover-V2-671B, Specializing in Mathematical Theorem Proving\nThe new model employs a Mixture of Experts (MoE) architecture and is trained using the Lean 4 framework for formal reasoning. With 671 billion parameters, it leverages reinforcement learning and large-scale synthetic data to significantly enhance automated theorem-proving capabilities.",
      "display_name": "Deepseek Prover V2 671B",
      "endpoints": [
        "completion"
      ],
      "id": "deepseek/deepseek-prover-v2-671b",
      "input_token_price_per_m": 7000,
      "max_output_tokens": 160000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 25000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "deepseek/deepseek-prover-v2-671b"
    },
    {
      "context_size": 131072,
      "created": 1743906925,
      "description": "Llama 4 Scout 17B Instruct (16E) is a mixture-of-experts (MoE) language model developed by Meta, activating 17 billion parameters out of a total of 109B. It supports native multimodal input (text and image) and multilingual output (text and code) across 12 supported languages. Designed for assistant-style interaction and visual reasoning, Scout uses 16 experts per forward pass and features a context length of 10 million tokens, with a training corpus of ~40 trillion tokens.\n\nBuilt for high efficiency and local or commercial deployment, Llama 4 Scout incorporates early fusion for seamless modality integration. It is instruction-tuned for use in multilingual chat, captioning, and image understanding tasks. Released under the Llama 4 Community License, it was last trained on data up to August 2024 and launched publicly on April 5, 2025.",
      "display_name": "Llama 4 Scout Instruct",
      "endpoints": [
        "completion"
      ],
      "features": [
        "function-calling",
        "vision"
      ],
      "id": "meta-llama/llama-4-scout-17b-16e-instruct",
      "input_token_price_per_m": 1000,
      "max_output_tokens": 131072,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 5000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "meta-llama/llama-4-scout-17b-16e-instruct"
    },
    {
      "context_size": 32000,
      "created": 1738498617,
      "description": "DeepSeek R1 Distill Llama 8B is a distilled large language model based on Llama-3.1-8B-Instruct, using outputs from DeepSeek R1. ",
      "display_name": "DeepSeek R1 Distill Llama 8B",
      "endpoints": [
        "completion"
      ],
      "features": [
        "structured-outputs"
      ],
      "id": "deepseek/deepseek-r1-distill-llama-8b",
      "input_token_price_per_m": 400,
      "max_output_tokens": 32000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 400,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "deepseek/deepseek-r1-distill-llama-8b"
    },
    {
      "context_size": 16384,
      "created": 1721801867,
      "description": "Meta's latest class of models, Llama 3.1, launched with a variety of sizes and configurations. The 8B instruct-tuned version is particularly fast and efficient. It has demonstrated strong performance in human evaluations, outperforming several leading closed-source models.",
      "display_name": "Llama 3.1 8B Instruct",
      "endpoints": [
        "completion"
      ],
      "id": "meta-llama/llama-3.1-8b-instruct",
      "input_token_price_per_m": 200,
      "max_output_tokens": 16384,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 500,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "meta-llama/llama-3.1-8b-instruct"
    },
    {
      "context_size": 64000,
      "created": 1738498371,
      "description": "DeepSeek R1 Distill Qwen 14B is a distilled large language model based on Qwen 2.5 14B, using outputs from DeepSeek R1. It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\nOther benchmark results include:\n\nAIME 2024 pass@1: 69.7\nMATH-500 pass@1: 93.9\nCodeForces Rating: 1481\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
      "display_name": "DeepSeek R1 Distill Qwen 14B",
      "endpoints": [
        "completion"
      ],
      "features": [
        "structured-outputs"
      ],
      "id": "deepseek/deepseek-r1-distill-qwen-14b",
      "input_token_price_per_m": 1500,
      "max_output_tokens": 32000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 1500,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "deepseek/deepseek-r1-distill-qwen-14b"
    },
    {
      "context_size": 131072,
      "created": 1733560109,
      "description": "The Meta Llama 3.3 multilingual large language model (LLM) is a pretrained and instruction tuned generative model in 70B (text in/text out). The Llama 3.3 instruction tuned text only model is optimized for multilingual dialogue use cases and outperforms many of the available open source and closed chat models on common industry benchmarks.\n\nSupported languages: English, German, French, Italian, Portuguese, Hindi, Spanish, and Thai.",
      "display_name": "Llama 3.3 70B Instruct",
      "endpoints": [
        "completion"
      ],
      "features": [
        "function-calling"
      ],
      "id": "meta-llama/llama-3.3-70b-instruct",
      "input_token_price_per_m": 1300,
      "max_output_tokens": 120000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 3900,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "meta-llama/llama-3.3-70b-instruct"
    },
    {
      "context_size": 32000,
      "created": 1728962258,
      "description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters.",
      "display_name": "Qwen 2.5 72B Instruct",
      "endpoints": [
        "completion"
      ],
      "features": [
        "structured-outputs",
        "function-calling"
      ],
      "id": "qwen/qwen-2.5-72b-instruct",
      "input_token_price_per_m": 3800,
      "max_output_tokens": 32000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 4000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "qwen/qwen-2.5-72b-instruct"
    },
    {
      "context_size": 60288,
      "created": 1722337858,
      "description": "A 12B parameter model with a 128k token context length built by Mistral in collaboration with NVIDIA. The model is multilingual, supporting English, French, German, Spanish, Italian, Portuguese, Chinese, Japanese, Korean, Arabic, and Hindi. It supports function calling and is released under the Apache 2.0 license.",
      "display_name": "Mistral Nemo",
      "endpoints": [
        "completion"
      ],
      "features": [
        "structured-outputs"
      ],
      "id": "mistralai/mistral-nemo",
      "input_token_price_per_m": 400,
      "max_output_tokens": 32000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 1700,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "mistralai/mistral-nemo"
    },
    {
      "context_size": 64000,
      "created": 1738498293,
      "description": "DeepSeek R1 Distill Qwen 32B is a distilled large language model based on Qwen 2.5 32B, using outputs from DeepSeek R1. It outperforms OpenAI's o1-mini across various benchmarks, achieving new state-of-the-art results for dense models.\n\nOther benchmark results include:\nAIME 2024 pass@1: 72.6\nMATH-500 pass@1: 94.3\nCodeForces Rating: 1691\nThe model leverages fine-tuning from DeepSeek R1's outputs, enabling competitive performance comparable to larger frontier models.",
      "display_name": "DeepSeek R1 Distill Qwen 32B",
      "endpoints": [
        "completion"
      ],
      "features": [
        "structured-outputs"
      ],
      "id": "deepseek/deepseek-r1-distill-qwen-32b",
      "input_token_price_per_m": 3000,
      "max_output_tokens": 32000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 3000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "deepseek/deepseek-r1-distill-qwen-32b"
    },
    {
      "context_size": 8192,
      "created": 1714024874,
      "description": "Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 8B instruct-tuned version was optimized for high quality dialogue usecases. It has demonstrated strong performance compared to leading closed-source models in human evaluations.",
      "display_name": "Llama 3 8B Instruct",
      "endpoints": [
        "completion"
      ],
      "id": "meta-llama/llama-3-8b-instruct",
      "input_token_price_per_m": 400,
      "max_output_tokens": 8192,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 400,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "meta-llama/llama-3-8b-instruct"
    },
    {
      "context_size": 65535,
      "created": 1713938472,
      "description": "WizardLM-2 8x22B is Microsoft AI's most advanced Wizard model. It demonstrates highly competitive performance compared to leading proprietary models, and it consistently outperforms all existing state-of-the-art opensource models.",
      "display_name": "Wizardlm 2 8x22B",
      "endpoints": [
        "completion"
      ],
      "id": "microsoft/wizardlm-2-8x22b",
      "input_token_price_per_m": 6200,
      "max_output_tokens": 8000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 6200,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "microsoft/wizardlm-2-8x22b"
    },
    {
      "context_size": 32000,
      "created": 1737957190,
      "description": "DeepSeek R1 Distill LLama 70B",
      "display_name": "DeepSeek R1 Distill LLama 70B",
      "endpoints": [
        "completion"
      ],
      "features": [
        "structured-outputs"
      ],
      "id": "deepseek/deepseek-r1-distill-llama-70b",
      "input_token_price_per_m": 8000,
      "max_output_tokens": 32000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 8000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "deepseek/deepseek-r1-distill-llama-70b"
    },
    {
      "context_size": 32768,
      "created": 1719492913,
      "description": "A high-performing, industry-standard 7.3B parameter model, with optimizations for speed and context length.",
      "display_name": "Mistral 7B Instruct",
      "endpoints": [
        "completion"
      ],
      "id": "mistralai/mistral-7b-instruct",
      "input_token_price_per_m": 290,
      "max_output_tokens": 8192,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 590,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "mistralai/mistral-7b-instruct"
    },
    {
      "context_size": 8192,
      "created": 1714024815,
      "description": "Meta's latest class of model (Llama 3) launched with a variety of sizes & flavors. This 70B instruct-tuned version was optimized for high quality dialogue usecases. It has demonstrated strong performance compared to leading closed-source models in human evaluations.",
      "display_name": "Llama3 70B Instruct",
      "endpoints": [
        "completion"
      ],
      "features": [
        "structured-outputs"
      ],
      "id": "meta-llama/llama-3-70b-instruct",
      "input_token_price_per_m": 5100,
      "max_output_tokens": 8000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 7400,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "meta-llama/llama-3-70b-instruct"
    },
    {
      "context_size": 8192,
      "created": 1719493012,
      "description": "Hermes 2 Pro is an upgraded, retrained version of Nous Hermes 2, consisting of an updated and cleaned version of the OpenHermes 2.5 Dataset, as well as a newly introduced Function Calling and JSON Mode dataset developed in-house.",
      "display_name": "Hermes 2 Pro Llama 3 8B",
      "endpoints": [
        "completion"
      ],
      "features": [
        "structured-outputs"
      ],
      "id": "nousresearch/hermes-2-pro-llama-3-8b",
      "input_token_price_per_m": 1400,
      "max_output_tokens": 8192,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 1400,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "nousresearch/hermes-2-pro-llama-3-8b"
    },
    {
      "context_size": 8192,
      "created": 1718699128,
      "description": "The uncensored llama3 model is a powerhouse of creativity, excelling in both roleplay and story writing. It offers a liberating experience during roleplays, free from any restrictions. This model stands out for its immense creativity, boasting a vast array of unique ideas and plots, truly a treasure trove for those seeking originality. Its unrestricted nature during roleplays allows for the full breadth of imagination to unfold, akin to an enhanced, big-brained version of Stheno. Perfect for creative minds seeking a boundless platform for their imaginative expressions, the uncensored llama3 model is an ideal choice",
      "display_name": "L3 70B Euryale V2.1\t",
      "endpoints": [
        "completion"
      ],
      "id": "sao10k/l3-70b-euryale-v2.1",
      "input_token_price_per_m": 14800,
      "max_output_tokens": 8192,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 14800,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "sao10k/l3-70b-euryale-v2.1"
    },
    {
      "context_size": 16000,
      "created": 1719492271,
      "description": "Dolphin 2.9 is designed for instruction following, conversational, and coding. This model is a finetune of Mixtral 8x22B Instruct. It features a 64k context length and was fine-tuned with a 16k sequence length using ChatML templates.The model is uncensored and is stripped of alignment and bias. It requires an external alignment layer for ethical use.",
      "display_name": "Dolphin Mixtral 8x22B",
      "endpoints": [
        "completion"
      ],
      "features": [
        "structured-outputs"
      ],
      "id": "cognitivecomputations/dolphin-mixtral-8x22b",
      "input_token_price_per_m": 9000,
      "max_output_tokens": 8192,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 9000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "cognitivecomputations/dolphin-mixtral-8x22b"
    },
    {
      "context_size": 4096,
      "created": 1718626452,
      "description": "A merge with a complex family tree, this model was crafted for roleplaying and storytelling. Midnight Rose is a successor to Rogue Rose and Aurora Nights and improves upon them both. It wants to produce lengthy output by default and is the best creative writing merge produced so far by sophosympatheia.",
      "display_name": "Midnight Rose 70B",
      "endpoints": [
        "completion"
      ],
      "id": "sophosympatheia/midnight-rose-70b",
      "input_token_price_per_m": 8000,
      "max_output_tokens": 2048,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 8000,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "sophosympatheia/midnight-rose-70b"
    },
    {
      "context_size": 8192,
      "created": 1732791714,
      "description": "A generalist / roleplaying model merge based on Llama 3.",
      "display_name": "Sao10k L3 8B Lunaris\t",
      "endpoints": [
        "completion"
      ],
      "features": [
        "structured-outputs"
      ],
      "id": "sao10k/l3-8b-lunaris",
      "input_token_price_per_m": 500,
      "max_output_tokens": 8192,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 500,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "sao10k/l3-8b-lunaris"
    },
    {
      "context_size": 30000,
      "created": 1751258754,
      "description": "The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio",
      "display_name": "ERNIE 4.5 VL 28B A3B",
      "features": [
        "function-calling",
        "vision"
      ],
      "id": "baidu/ernie-4.5-vl-28b-a3b",
      "input_token_price_per_m": 0,
      "max_output_tokens": 8000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 0,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "baidu/ernie-4.5-vl-28b-a3b"
    },
    {
      "context_size": 120000,
      "created": 1751258682,
      "description": "The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio",
      "display_name": "ERNIE 4.5 21B A3B",
      "endpoints": [
        "completion"
      ],
      "features": [
        "function-calling"
      ],
      "id": "baidu/ernie-4.5-21B-a3b",
      "input_token_price_per_m": 0,
      "max_output_tokens": 8000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 0,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "baidu/ernie-4.5-21B-a3b"
    },
    {
      "context_size": 120000,
      "created": 1751258572,
      "description": "The ERNIE 4.5 series of open-source models adopts a Mixture-of-Experts (MoE) architecture, representing an innovative multimodal heterogeneous model structure. It achieves cross-modal knowledge fusion through a parameter-sharing mechanism while retaining dedicated parameter spaces for individual modalities. This architecture is particularly well-suited for the continuous pre-training paradigm from large language models to multimodal models, significantly enhancing multimodal understanding capabilities while maintaining or even improving performance in text-based tasks. The models are efficiently trained, inferred, and deployed using the PaddlePaddle deep learning framework. During the pre-training of large language models, the Model FLOPs Utilization (MFU) reaches 47%. Experimental results demonstrate that this series of models achieves state-of-the-art (SOTA) performance across multiple text and multimodal benchmarks, with particularly outstanding results in instruction following, world knowledge memorizatio",
      "display_name": "ERNIE 4.5 0.3B",
      "endpoints": [
        "completion"
      ],
      "features": [
        "function-calling"
      ],
      "id": "baidu/ernie-4.5-0.3b",
      "input_token_price_per_m": 0,
      "max_output_tokens": 8000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 0,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "baidu/ernie-4.5-0.3b"
    },
    {
      "context_size": 32768,
      "created": 1750848096,
      "description": "Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models. Gemma 3 models are multimodal, handling text and image input and generating text output, with open weights for both pre-trained variants and instruction-tuned variants. Gemma 3 has a large, 128K context window, multilingual support in over 140 languages, and is available in more sizes than previous versions. Gemma 3 models are well-suited for a variety of text generation and image understanding tasks, including question answering, summarization, and reasoning. Their relatively small size makes it possible to deploy them in environments with limited resources such as laptops, desktops or your own cloud infrastructure, democratizing access to state of the art AI models and helping foster innovation for everyone.",
      "display_name": "Gemma3 1B IT",
      "id": "google/gemma-3-1b-it",
      "input_token_price_per_m": 0,
      "max_output_tokens": 131072,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 0,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "google/gemma-3-1b-it"
    },
    {
      "context_size": 128000,
      "created": 1745901633,
      "description": "Achieves effective integration of reasoning and non-reasoning modes, allowing seamless mode switching during conversations. Its reasoning capability reaches state-of-the-art (SOTA) performance among models of the same scale, and its general capabilities significantly outperform those of Qwen2.5-7B.",
      "display_name": "Qwen3 8B",
      "endpoints": [
        "completion"
      ],
      "id": "qwen/qwen3-8b-fp8",
      "input_token_price_per_m": 350,
      "max_output_tokens": 20000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 1380,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "qwen/qwen3-8b-fp8"
    },
    {
      "context_size": 128000,
      "created": 1745901504,
      "description": "Achieves effective integration of reasoning and non-reasoning modes, allowing seamless switching during conversations. The model delivers state-of-the-art (SOTA) reasoning performance among models of the same scale, with significantly enhanced human preference alignment. Notable improvements are seen in creative writing, role-playing, multi-turn dialogue, and instruction following, leading to a clearly improved user experience.",
      "display_name": "Qwen3 4B",
      "endpoints": [
        "completion"
      ],
      "id": "qwen/qwen3-4b-fp8",
      "input_token_price_per_m": 0,
      "max_output_tokens": 20000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 0,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "qwen/qwen3-4b-fp8"
    },
    {
      "context_size": 32000,
      "created": 1744798076,
      "description": "GLM-4-32B-0414 is the latest open-source model in the GLM series, featuring 32 billion parameters. Its performance is comparable to OpenAI's GPT series and DeepSeek's V3/R1 series, while also supporting highly user-friendly local deployment capabilities.\nGLM-4-32B-Base-0414 was pre-trained on 15T of high-quality data, including a large amount of reasoning-type synthetic data, which laid a solid foundation for subsequent reinforcement learning extensions. In the post-training stage, in addition to human preference alignment for dialogue scenarios, the research team enhanced the model’s performance in instruction following, engineering code, and function calling using techniques such as rejection sampling and reinforcement learning, thereby strengthening the atomic capabilities required for agent tasks.\nGLM-4-32B-0414 has achieved strong results in engineering code generation, artifact creation, function calling, search-based question answering, and report generation. On several benchmarks, its performance appr",
      "display_name": "GLM-4-32B-0414",
      "endpoints": [
        "completion"
      ],
      "features": [
        "function-calling",
        "structured-outputs"
      ],
      "id": "thudm/glm-4-32b-0414",
      "input_token_price_per_m": 2400,
      "max_output_tokens": 32000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 2400,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "thudm/glm-4-32b-0414"
    },
    {
      "context_size": 32000,
      "created": 1744797581,
      "description": "Qwen2.5 is the latest series of Qwen large language models. For Qwen2.5, we release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters. Qwen2.5 brings the following improvements upon Qwen2:\n- Significantly more knowledge and has greatly improved capabilities in coding and mathematics, thanks to our specialized expert models in these domains.\n- Significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON. More resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.\n- Long-context Support up to 128K tokens and can generate up to 8K tokens.\n- Multilingual support for over 29 languages, including Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, Arabic, and more.",
      "display_name": "Qwen2.5 7B Instruct",
      "endpoints": [
        "completion"
      ],
      "features": [
        "function-calling",
        "structured-outputs"
      ],
      "id": "qwen/qwen2.5-7b-instruct",
      "input_token_price_per_m": 0,
      "max_output_tokens": 32000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 0,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "qwen/qwen2.5-7b-instruct"
    },
    {
      "context_size": 131000,
      "created": 1732609540,
      "description": "The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out).",
      "display_name": "Llama 3.2 1B Instruct\t",
      "endpoints": [
        "completion"
      ],
      "id": "meta-llama/llama-3.2-1b-instruct",
      "input_token_price_per_m": 0,
      "max_output_tokens": 32000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 0,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "meta-llama/llama-3.2-1b-instruct"
    },
    {
      "context_size": 32768,
      "created": 1732607748,
      "description": "The Meta Llama 3.2 collection of multilingual large language models (LLMs) is a collection of pretrained and instruction-tuned generative models in 1B and 3B sizes (text in/text out)",
      "display_name": "Llama 3.2 3B Instruct",
      "endpoints": [
        "completion"
      ],
      "features": [
        "function-calling"
      ],
      "id": "meta-llama/llama-3.2-3b-instruct",
      "input_token_price_per_m": 300,
      "max_output_tokens": 32000,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 500,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "meta-llama/llama-3.2-3b-instruct"
    },
    {
      "context_size": 8192,
      "created": 1726742141,
      "description": "Euryale L3.1 70B v2.2 is a model focused on creative roleplay from Sao10k. It is the successor of Euryale L3 70B v2.1.",
      "display_name": "L31 70B Euryale V2.2",
      "endpoints": [
        "completion"
      ],
      "id": "sao10k/l31-70b-euryale-v2.2",
      "input_token_price_per_m": 14800,
      "max_output_tokens": 8192,
      "model_type": "chat",
      "object": "model",
      "output_token_price_per_m": 14800,
      "owned_by": "unknown",
      "parent": "",
      "permission": null,
      "root": "",
      "status": 1,
      "tags": [],
      "title": "sao10k/l31-70b-euryale-v2.2"
    }
  ]
}