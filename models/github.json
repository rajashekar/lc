[
  {
    "capabilities": [
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azure-openai/gpt-4-1",
    "id": "openai/gpt-4.1",
    "limits": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 32768
    },
    "name": "OpenAI GPT-4.1",
    "publisher": "OpenAI",
    "rate_limit_tier": "high",
    "registry": "azure-openai",
    "summary": "gpt-4.1 outperforms gpt-4o across the board, with major gains in coding, instruction following, and long-context understanding",
    "supported_input_modalities": [
      "text",
      "image"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "multipurpose",
      "multilingual",
      "multimodal"
    ],
    "version": "2025-04-14"
  },
  {
    "capabilities": [
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azure-openai/gpt-4-1-mini",
    "id": "openai/gpt-4.1-mini",
    "limits": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 32768
    },
    "name": "OpenAI GPT-4.1-mini",
    "publisher": "OpenAI",
    "rate_limit_tier": "low",
    "registry": "azure-openai",
    "summary": "gpt-4.1-mini outperform gpt-4o-mini across the board, with major gains in coding, instruction following, and long-context handling",
    "supported_input_modalities": [
      "text",
      "image"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "multipurpose",
      "multilingual",
      "multimodal"
    ],
    "version": "2025-04-14"
  },
  {
    "capabilities": [
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azure-openai/gpt-4-1-nano",
    "id": "openai/gpt-4.1-nano",
    "limits": {
      "max_input_tokens": 1048576,
      "max_output_tokens": 32768
    },
    "name": "OpenAI GPT-4.1-nano",
    "publisher": "OpenAI",
    "rate_limit_tier": "low",
    "registry": "azure-openai",
    "summary": "gpt-4.1-nano provides gains in coding, instruction following, and long-context handling along with lower latency and cost",
    "supported_input_modalities": [
      "text",
      "image"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "multipurpose",
      "multilingual",
      "multimodal"
    ],
    "version": "2025-04-14"
  },
  {
    "capabilities": [
      "agents",
      "assistants",
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azure-openai/gpt-4o",
    "id": "openai/gpt-4o",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 16384
    },
    "name": "OpenAI GPT-4o",
    "publisher": "OpenAI",
    "rate_limit_tier": "high",
    "registry": "azure-openai",
    "summary": "OpenAI's most advanced multimodal model in the gpt-4o family. Can handle both text and image inputs.",
    "supported_input_modalities": [
      "text",
      "image",
      "audio"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "multipurpose",
      "multilingual",
      "multimodal"
    ],
    "version": "2024-11-20"
  },
  {
    "capabilities": [
      "agents",
      "assistants",
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azure-openai/gpt-4o-mini",
    "id": "openai/gpt-4o-mini",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "OpenAI GPT-4o mini",
    "publisher": "OpenAI",
    "rate_limit_tier": "low",
    "registry": "azure-openai",
    "summary": "An affordable, efficient AI solution for diverse text and image tasks.",
    "supported_input_modalities": [
      "text",
      "image",
      "audio"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "multipurpose",
      "multilingual",
      "multimodal"
    ],
    "version": "2024-07-18"
  },
  {
    "capabilities": [
      "reasoning",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azure-openai/o1",
    "id": "openai/o1",
    "limits": {
      "max_input_tokens": 200000,
      "max_output_tokens": 100000
    },
    "name": "OpenAI o1",
    "publisher": "OpenAI",
    "rate_limit_tier": "custom",
    "registry": "azure-openai",
    "summary": "Focused on advanced reasoning and solving complex problems, including math and science tasks. Ideal for applications that require deep contextual understanding and agentic workflows.",
    "supported_input_modalities": [
      "text",
      "image"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "multilingual",
      "coding"
    ],
    "version": "2024-12-17"
  },
  {
    "capabilities": [
      "reasoning",
      "streaming"
    ],
    "html_url": "https://github.com/marketplace/models/azure-openai/o1-mini",
    "id": "openai/o1-mini",
    "limits": {
      "max_input_tokens": 128000,
      "max_output_tokens": 65536
    },
    "name": "OpenAI o1-mini",
    "publisher": "OpenAI",
    "rate_limit_tier": "custom",
    "registry": "azure-openai",
    "summary": "Smaller, faster, and 80% cheaper than o1-preview, performs well at code generation and small context operations.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "multilingual",
      "coding"
    ],
    "version": "2024-09-12"
  },
  {
    "capabilities": [
      "reasoning"
    ],
    "html_url": "https://github.com/marketplace/models/azure-openai/o1-preview",
    "id": "openai/o1-preview",
    "limits": {
      "max_input_tokens": 128000,
      "max_output_tokens": 32768
    },
    "name": "OpenAI o1-preview",
    "publisher": "OpenAI",
    "rate_limit_tier": "custom",
    "registry": "azure-openai",
    "summary": "Focused on advanced reasoning and solving complex problems, including math and science tasks. Ideal for applications that require deep contextual understanding and agentic workflows.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "multilingual",
      "coding"
    ],
    "version": "1"
  },
  {
    "capabilities": [
      "reasoning",
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azure-openai/o3",
    "id": "openai/o3",
    "limits": {
      "max_input_tokens": 200000,
      "max_output_tokens": 100000
    },
    "name": "OpenAI o3",
    "publisher": "OpenAI",
    "rate_limit_tier": "custom",
    "registry": "azure-openai",
    "summary": "o3 includes significant improvements on quality and safety while supporting the existing features of o1 and delivering comparable or better performance.",
    "supported_input_modalities": [
      "text",
      "image"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "multipurpose",
      "multilingual",
      "multimodal"
    ],
    "version": "2025-04-16"
  },
  {
    "capabilities": [
      "reasoning",
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azure-openai/o3-mini",
    "id": "openai/o3-mini",
    "limits": {
      "max_input_tokens": 200000,
      "max_output_tokens": 100000
    },
    "name": "OpenAI o3-mini",
    "publisher": "OpenAI",
    "rate_limit_tier": "custom",
    "registry": "azure-openai",
    "summary": "o3-mini includes the o1 features with significant cost-efficiencies for scenarios requiring high performance.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "multilingual",
      "coding"
    ],
    "version": "2025-01-31"
  },
  {
    "capabilities": [
      "reasoning",
      "tool-calling",
      "streaming"
    ],
    "html_url": "https://github.com/marketplace/models/azure-openai/o4-mini",
    "id": "openai/o4-mini",
    "limits": {
      "max_input_tokens": 200000,
      "max_output_tokens": 100000
    },
    "name": "OpenAI o4-mini",
    "publisher": "OpenAI",
    "rate_limit_tier": "custom",
    "registry": "azure-openai",
    "summary": "o4-mini includes significant improvements on quality and safety while supporting the existing features of o3-mini and delivering comparable or better performance.",
    "supported_input_modalities": [
      "text",
      "image"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "multipurpose",
      "multilingual",
      "multimodal"
    ],
    "version": "2025-04-16"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azure-openai/text-embedding-3-large",
    "id": "openai/text-embedding-3-large",
    "limits": {
      "max_input_tokens": 8191,
      "max_output_tokens": null
    },
    "name": "OpenAI Text Embedding 3 (large)",
    "publisher": "OpenAI",
    "rate_limit_tier": "embeddings",
    "registry": "azure-openai",
    "summary": "Text-embedding-3 series models are the latest and most capable embedding model from OpenAI.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "embeddings"
    ],
    "tags": [
      "rag"
    ],
    "version": "1"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azure-openai/text-embedding-3-small",
    "id": "openai/text-embedding-3-small",
    "limits": {
      "max_input_tokens": 8191,
      "max_output_tokens": null
    },
    "name": "OpenAI Text Embedding 3 (small)",
    "publisher": "OpenAI",
    "rate_limit_tier": "embeddings",
    "registry": "azure-openai",
    "summary": "Text-embedding-3 series models are the latest and most capable embedding model from OpenAI.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "embeddings"
    ],
    "tags": [
      "rag"
    ],
    "version": "1"
  },
  {
    "capabilities": [
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-ai21/AI21-Jamba-1-5-Large",
    "id": "ai21-labs/ai21-jamba-1.5-large",
    "limits": {
      "max_input_tokens": 262144,
      "max_output_tokens": 4096
    },
    "name": "AI21 Jamba 1.5 Large",
    "publisher": "AI21 Labs",
    "rate_limit_tier": "high",
    "registry": "azureml-ai21",
    "summary": "A 398B parameters (94B active) multilingual model, offering a 256K long context window, function calling, structured output, and grounded generation.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "rag",
      "multilingual",
      "large context"
    ],
    "version": "1"
  },
  {
    "capabilities": [
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-ai21/AI21-Jamba-1-5-Mini",
    "id": "ai21-labs/ai21-jamba-1.5-mini",
    "limits": {
      "max_input_tokens": 262144,
      "max_output_tokens": 4096
    },
    "name": "AI21 Jamba 1.5 Mini",
    "publisher": "AI21 Labs",
    "rate_limit_tier": "low",
    "registry": "azureml-ai21",
    "summary": "A 52B parameters (12B active) multilingual model, offering a 256K long context window, function calling, structured output, and grounded generation.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "rag",
      "multilingual",
      "large context"
    ],
    "version": "1"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml-cohere/cohere-command-a",
    "id": "cohere/cohere-command-a",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Cohere Command A",
    "publisher": "Cohere",
    "rate_limit_tier": "low",
    "registry": "azureml-cohere",
    "summary": "Command A is a highly efficient generative model that excels at agentic and multilingual use cases.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "rag",
      "multilingual"
    ],
    "version": "3"
  },
  {
    "capabilities": [
      "streaming"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-cohere/Cohere-command-r-08-2024",
    "id": "cohere/cohere-command-r-08-2024",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Cohere Command R 08-2024",
    "publisher": "Cohere",
    "rate_limit_tier": "low",
    "registry": "azureml-cohere",
    "summary": "Command R is a scalable generative model targeting RAG and Tool Use to enable production-scale AI for enterprise.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "rag",
      "multilingual"
    ],
    "version": "1"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml-cohere/Cohere-command-r-plus-08-2024",
    "id": "cohere/cohere-command-r-plus-08-2024",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Cohere Command R+ 08-2024",
    "publisher": "Cohere",
    "rate_limit_tier": "high",
    "registry": "azureml-cohere",
    "summary": "Command R+ is a state-of-the-art RAG-optimized model designed to tackle enterprise-grade workloads.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "rag",
      "multilingual"
    ],
    "version": "1"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml-cohere/Cohere-embed-v3-english",
    "id": "cohere/cohere-embed-v3-english",
    "limits": {
      "max_input_tokens": 512,
      "max_output_tokens": null
    },
    "name": "Cohere Embed v3 English",
    "publisher": "Cohere",
    "rate_limit_tier": "embeddings",
    "registry": "azureml-cohere",
    "summary": "Cohere Embed English is the market's leading text representation model used for semantic search, retrieval-augmented generation (RAG), classification, and clustering.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "embeddings"
    ],
    "tags": [
      "rag"
    ],
    "version": "1"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml-cohere/Cohere-embed-v3-multilingual",
    "id": "cohere/cohere-embed-v3-multilingual",
    "limits": {
      "max_input_tokens": 512,
      "max_output_tokens": null
    },
    "name": "Cohere Embed v3 Multilingual",
    "publisher": "Cohere",
    "rate_limit_tier": "embeddings",
    "registry": "azureml-cohere",
    "summary": "Cohere Embed Multilingual is the market's leading text representation model used for semantic search, retrieval-augmented generation (RAG), classification, and clustering.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "embeddings"
    ],
    "tags": [
      "rag"
    ],
    "version": "1"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml-core42/jais-30b-chat",
    "id": "core42/jais-30b-chat",
    "limits": {
      "max_input_tokens": 8192,
      "max_output_tokens": 4096
    },
    "name": "JAIS 30b Chat",
    "publisher": "Core42",
    "rate_limit_tier": "low",
    "registry": "azureml-core42",
    "summary": "JAIS 30b Chat is an auto-regressive bilingual LLM for Arabic & English with state-of-the-art capabilities in Arabic.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "conversation",
      "multilingual",
      "rag"
    ],
    "version": "3"
  },
  {
    "capabilities": [
      "reasoning",
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-deepseek/DeepSeek-R1",
    "id": "deepseek/deepseek-r1",
    "limits": {
      "max_input_tokens": 128000,
      "max_output_tokens": 4096
    },
    "name": "DeepSeek-R1",
    "publisher": "DeepSeek",
    "rate_limit_tier": "custom",
    "registry": "azureml-deepseek",
    "summary": "DeepSeek-R1 excels at reasoning tasks using a step-by-step training process, such as language, scientific reasoning, and coding tasks.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "coding",
      "agents"
    ],
    "version": "1"
  },
  {
    "capabilities": [
      "reasoning",
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-deepseek/DeepSeek-R1-0528",
    "id": "deepseek/deepseek-r1-0528",
    "limits": {
      "max_input_tokens": 128000,
      "max_output_tokens": 4096
    },
    "name": "DeepSeek-R1-0528",
    "publisher": "DeepSeek",
    "rate_limit_tier": "custom",
    "registry": "azureml-deepseek",
    "summary": "The DeepSeek R1 0528 model has improved reasoning capabilities, this version also offers a reduced hallucination rate, enhanced support for function calling, and better experience for vibe coding.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "coding",
      "agents"
    ],
    "version": "1"
  },
  {
    "capabilities": [
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-deepseek/DeepSeek-V3-0324",
    "id": "deepseek/deepseek-v3-0324",
    "limits": {
      "max_input_tokens": 128000,
      "max_output_tokens": 4096
    },
    "name": "DeepSeek-V3-0324",
    "publisher": "DeepSeek",
    "rate_limit_tier": "high",
    "registry": "azureml-deepseek",
    "summary": "DeepSeek-V3-0324 demonstrates notable improvements over its predecessor, DeepSeek-V3, in several key aspects, including enhanced reasoning, improved function calling, and superior code generation capabilities.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "coding",
      "agents"
    ],
    "version": "1"
  },
  {
    "capabilities": [
      "streaming"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-meta/Llama-3-2-11B-Vision-Instruct",
    "id": "meta/llama-3.2-11b-vision-instruct",
    "limits": {
      "max_input_tokens": 128000,
      "max_output_tokens": 4096
    },
    "name": "Llama-3.2-11B-Vision-Instruct",
    "publisher": "Meta",
    "rate_limit_tier": "low",
    "registry": "azureml-meta",
    "summary": "Excels in image reasoning capabilities on high-res images for visual understanding apps.",
    "supported_input_modalities": [
      "text",
      "image",
      "audio"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "multimodal",
      "reasoning",
      "conversation"
    ],
    "version": "6"
  },
  {
    "capabilities": [
      "streaming"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-meta/Llama-3-2-90B-Vision-Instruct",
    "id": "meta/llama-3.2-90b-vision-instruct",
    "limits": {
      "max_input_tokens": 128000,
      "max_output_tokens": 4096
    },
    "name": "Llama-3.2-90B-Vision-Instruct",
    "publisher": "Meta",
    "rate_limit_tier": "high",
    "registry": "azureml-meta",
    "summary": "Advanced image reasoning capabilities for visual understanding agentic apps.",
    "supported_input_modalities": [
      "text",
      "image",
      "audio"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "multimodal",
      "reasoning",
      "conversation"
    ],
    "version": "5"
  },
  {
    "capabilities": [
      "streaming"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-meta/Llama-3-3-70B-Instruct",
    "id": "meta/llama-3.3-70b-instruct",
    "limits": {
      "max_input_tokens": 128000,
      "max_output_tokens": 4096
    },
    "name": "Llama-3.3-70B-Instruct",
    "publisher": "Meta",
    "rate_limit_tier": "high",
    "registry": "azureml-meta",
    "summary": "Llama 3.3 70B Instruct offers enhanced reasoning, math, and instruction following with performance comparable to Llama 3.1 405B.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "conversation"
    ],
    "version": "7"
  },
  {
    "capabilities": [
      "agents",
      "assistants",
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-meta/Llama-4-Maverick-17B-128E-Instruct-FP8",
    "id": "meta/llama-4-maverick-17b-128e-instruct-fp8",
    "limits": {
      "max_input_tokens": 1000000,
      "max_output_tokens": 4096
    },
    "name": "Llama 4 Maverick 17B 128E Instruct FP8",
    "publisher": "Meta",
    "rate_limit_tier": "high",
    "registry": "azureml-meta",
    "summary": "Llama 4 Maverick 17B 128E Instruct FP8 is great at precise image understanding and creative writing, offering high quality at a lower price compared to Llama 3.3 70B",
    "supported_input_modalities": [
      "text",
      "image"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "multimodal",
      "conversation",
      "multilingual"
    ],
    "version": "1"
  },
  {
    "capabilities": [
      "agents",
      "assistants",
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-meta/Llama-4-Scout-17B-16E-Instruct",
    "id": "meta/llama-4-scout-17b-16e-instruct",
    "limits": {
      "max_input_tokens": 10000000,
      "max_output_tokens": 4096
    },
    "name": "Llama 4 Scout 17B 16E Instruct",
    "publisher": "Meta",
    "rate_limit_tier": "high",
    "registry": "azureml-meta",
    "summary": "Llama 4 Scout 17B 16E Instruct is great at multi-document summarization, parsing extensive user activity for personalized tasks, and reasoning over vast codebases.",
    "supported_input_modalities": [
      "text",
      "image"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "multimodal",
      "conversation",
      "multilingual"
    ],
    "version": "2"
  },
  {
    "capabilities": [
      "agents"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-meta/Meta-Llama-3-1-405B-Instruct",
    "id": "meta/meta-llama-3.1-405b-instruct",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Meta-Llama-3.1-405B-Instruct",
    "publisher": "Meta",
    "rate_limit_tier": "high",
    "registry": "azureml-meta",
    "summary": "The Llama 3.1 instruction tuned text only models are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "conversation"
    ],
    "version": "1"
  },
  {
    "capabilities": [
      "streaming"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-meta/Meta-Llama-3-1-8B-Instruct",
    "id": "meta/meta-llama-3.1-8b-instruct",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Meta-Llama-3.1-8B-Instruct",
    "publisher": "Meta",
    "rate_limit_tier": "low",
    "registry": "azureml-meta",
    "summary": "The Llama 3.1 instruction tuned text only models are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "conversation"
    ],
    "version": "5"
  },
  {
    "capabilities": [
      "streaming"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-mistral/Codestral-2501",
    "id": "mistral-ai/codestral-2501",
    "limits": {
      "max_input_tokens": 256000,
      "max_output_tokens": 4096
    },
    "name": "Codestral 25.01",
    "publisher": "Mistral AI",
    "rate_limit_tier": "low",
    "registry": "azureml-mistral",
    "summary": "Codestral 25.01 by Mistral AI is designed for code generation, supporting 80+ programming languages, and optimized for tasks like code completion and fill-in-the-middle",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "coding"
    ],
    "version": "2"
  },
  {
    "capabilities": [
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-mistral/Ministral-3B",
    "id": "mistral-ai/ministral-3b",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Ministral 3B",
    "publisher": "Mistral AI",
    "rate_limit_tier": "low",
    "registry": "azureml-mistral",
    "summary": "Ministral 3B is a state-of-the-art Small Language Model (SLM) optimized for edge computing and on-device applications. As it is designed for low-latency and compute-efficient inference, it it also the perfect model for standard GenAI applications that have",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "low latency",
      "agents",
      "reasoning"
    ],
    "version": "1"
  },
  {
    "capabilities": [
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-mistral/Mistral-Large-2411",
    "id": "mistral-ai/mistral-large-2411",
    "limits": {
      "max_input_tokens": 128000,
      "max_output_tokens": 4096
    },
    "name": "Mistral Large 24.11",
    "publisher": "Mistral AI",
    "rate_limit_tier": "high",
    "registry": "azureml-mistral",
    "summary": "Mistral Large 24.11 offers enhanced system prompts, advanced reasoning and function calling capabilities.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "rag",
      "agents"
    ],
    "version": "2"
  },
  {
    "capabilities": [
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-mistral/mistral-medium-2505",
    "id": "mistral-ai/mistral-medium-2505",
    "limits": {
      "max_input_tokens": 128000,
      "max_output_tokens": 4096
    },
    "name": "Mistral Medium 3 (25.05)",
    "publisher": "Mistral AI",
    "rate_limit_tier": "low",
    "registry": "azureml-mistral",
    "summary": "Mistral Medium 3 is an advanced Large Language Model (LLM) with state-of-the-art reasoning, knowledge, coding and vision capabilities.",
    "supported_input_modalities": [
      "text",
      "image"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "multipurpose",
      "vision"
    ],
    "version": "1"
  },
  {
    "capabilities": [
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-mistral/Mistral-Nemo",
    "id": "mistral-ai/mistral-nemo",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Mistral Nemo",
    "publisher": "Mistral AI",
    "rate_limit_tier": "low",
    "registry": "azureml-mistral",
    "summary": "Mistral Nemo is a cutting-edge Language Model (LLM) boasting state-of-the-art reasoning, world knowledge, and coding capabilities within its size category.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "rag",
      "agents"
    ],
    "version": "1"
  },
  {
    "capabilities": [
      "agents",
      "assistants",
      "streaming",
      "tool-calling"
    ],
    "html_url": "https://github.com/marketplace/models/azureml-mistral/mistral-small-2503",
    "id": "mistral-ai/mistral-small-2503",
    "limits": {
      "max_input_tokens": 128000,
      "max_output_tokens": 4096
    },
    "name": "Mistral Small 3.1",
    "publisher": "Mistral AI",
    "rate_limit_tier": "low",
    "registry": "azureml-mistral",
    "summary": "Enhanced Mistral Small 3 with multimodal capabilities and a 128k context length.",
    "supported_input_modalities": [
      "text",
      "image"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "multipurpose",
      "vision",
      "multimodal"
    ],
    "version": "1"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml-xai/grok-3",
    "id": "xai/grok-3",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Grok 3",
    "publisher": "xAI",
    "rate_limit_tier": "custom",
    "registry": "azureml-xai",
    "summary": "Grok 3 is xAI's debut model, pretrained by Colossus at supermassive scale to excel in specialized domains like finance, healthcare, and the law.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "understanding",
      "instruction",
      "summarization"
    ],
    "version": "1"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml-xai/grok-3-mini",
    "id": "xai/grok-3-mini",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Grok 3 Mini",
    "publisher": "xAI",
    "rate_limit_tier": "custom",
    "registry": "azureml-xai",
    "summary": "Grok 3 Mini is a lightweight model that thinks before responding. Trained on mathematic and scientific problems, it is great for logic-based tasks.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "agents",
      "reasoning",
      "coding"
    ],
    "version": "1"
  },
  {
    "capabilities": [
      "reasoning",
      "streaming"
    ],
    "html_url": "https://github.com/marketplace/models/azureml/MAI-DS-R1",
    "id": "microsoft/mai-ds-r1",
    "limits": {
      "max_input_tokens": 128000,
      "max_output_tokens": 4096
    },
    "name": "MAI-DS-R1",
    "publisher": "Microsoft",
    "rate_limit_tier": "custom",
    "registry": "azureml",
    "summary": "MAI-DS-R1 is a DeepSeek-R1 reasoning model that has been post-trained by the Microsoft AI team to fill in information gaps in the previous version of the model and improve its harm protections while maintaining R1 reasoning capabilities.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "coding",
      "agents"
    ],
    "version": "1"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml/Phi-3-5-mini-instruct",
    "id": "microsoft/phi-3.5-mini-instruct",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Phi-3.5-mini instruct (128k)",
    "publisher": "Microsoft",
    "rate_limit_tier": "low",
    "registry": "azureml",
    "summary": "Refresh of Phi-3-mini model.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "understanding",
      "low latency"
    ],
    "version": "6"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml/Phi-3-5-MoE-instruct",
    "id": "microsoft/phi-3.5-moe-instruct",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Phi-3.5-MoE instruct (128k)",
    "publisher": "Microsoft",
    "rate_limit_tier": "low",
    "registry": "azureml",
    "summary": "A new mixture of experts model",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "understanding",
      "low latency"
    ],
    "version": "5"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml/Phi-3-5-vision-instruct",
    "id": "microsoft/phi-3.5-vision-instruct",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Phi-3.5-vision instruct (128k)",
    "publisher": "Microsoft",
    "rate_limit_tier": "low",
    "registry": "azureml",
    "summary": "Refresh of Phi-3-vision model.",
    "supported_input_modalities": [
      "text",
      "image"
    ],
    "supported_output_modalities": [],
    "tags": [
      "multimodal",
      "reasoning",
      "low latency"
    ],
    "version": "2"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml/Phi-3-medium-128k-instruct",
    "id": "microsoft/phi-3-medium-128k-instruct",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Phi-3-medium instruct (128k)",
    "publisher": "Microsoft",
    "rate_limit_tier": "low",
    "registry": "azureml",
    "summary": "Same Phi-3-medium model, but with a larger context size for RAG or few shot prompting.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "understanding",
      "large context"
    ],
    "version": "7"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml/Phi-3-medium-4k-instruct",
    "id": "microsoft/phi-3-medium-4k-instruct",
    "limits": {
      "max_input_tokens": 4096,
      "max_output_tokens": 4096
    },
    "name": "Phi-3-medium instruct (4k)",
    "publisher": "Microsoft",
    "rate_limit_tier": "low",
    "registry": "azureml",
    "summary": "A 14B parameters model, proves better quality than Phi-3-mini, with a focus on high-quality, reasoning-dense data.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "understanding"
    ],
    "version": "6"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml/Phi-3-mini-128k-instruct",
    "id": "microsoft/phi-3-mini-128k-instruct",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Phi-3-mini instruct (128k)",
    "publisher": "Microsoft",
    "rate_limit_tier": "low",
    "registry": "azureml",
    "summary": "Same Phi-3-mini model, but with a larger context size for RAG or few shot prompting.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "understanding",
      "low latency"
    ],
    "version": "13"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml/Phi-3-mini-4k-instruct",
    "id": "microsoft/phi-3-mini-4k-instruct",
    "limits": {
      "max_input_tokens": 4096,
      "max_output_tokens": 4096
    },
    "name": "Phi-3-mini instruct (4k)",
    "publisher": "Microsoft",
    "rate_limit_tier": "low",
    "registry": "azureml",
    "summary": "Tiniest member of the Phi-3 family. Optimized for both quality and low latency.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "understanding",
      "low latency"
    ],
    "version": "15"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml/Phi-3-small-128k-instruct",
    "id": "microsoft/phi-3-small-128k-instruct",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Phi-3-small instruct (128k)",
    "publisher": "Microsoft",
    "rate_limit_tier": "low",
    "registry": "azureml",
    "summary": "Same Phi-3-small model, but with a larger context size for RAG or few shot prompting.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "understanding",
      "large context"
    ],
    "version": "5"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml/Phi-3-small-8k-instruct",
    "id": "microsoft/phi-3-small-8k-instruct",
    "limits": {
      "max_input_tokens": 131072,
      "max_output_tokens": 4096
    },
    "name": "Phi-3-small instruct (8k)",
    "publisher": "Microsoft",
    "rate_limit_tier": "low",
    "registry": "azureml",
    "summary": "A 7B parameters model, proves better quality than Phi-3-mini, with a focus on high-quality, reasoning-dense data.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "understanding"
    ],
    "version": "6"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml/Phi-4",
    "id": "microsoft/phi-4",
    "limits": {
      "max_input_tokens": 16384,
      "max_output_tokens": 16384
    },
    "name": "Phi-4",
    "publisher": "Microsoft",
    "rate_limit_tier": "low",
    "registry": "azureml",
    "summary": "Phi-4 14B, a highly capable model for low latency scenarios.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "understanding",
      "low latency"
    ],
    "version": "8"
  },
  {
    "capabilities": [],
    "html_url": "https://github.com/marketplace/models/azureml/Phi-4-mini-instruct",
    "id": "microsoft/phi-4-mini-instruct",
    "limits": {
      "max_input_tokens": 128000,
      "max_output_tokens": 4096
    },
    "name": "Phi-4-mini-instruct",
    "publisher": "Microsoft",
    "rate_limit_tier": "low",
    "registry": "azureml",
    "summary": "3.8B parameters Small Language Model outperforming larger models in reasoning, math, coding, and function-calling",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "agents",
      "multilingual"
    ],
    "version": "1"
  },
  {
    "capabilities": [
      "reasoning"
    ],
    "html_url": "https://github.com/marketplace/models/azureml/Phi-4-mini-reasoning",
    "id": "microsoft/phi-4-mini-reasoning",
    "limits": {
      "max_input_tokens": 128000,
      "max_output_tokens": 4096
    },
    "name": "Phi-4-mini-reasoning",
    "publisher": "Microsoft",
    "rate_limit_tier": "low",
    "registry": "azureml",
    "summary": "Lightweight math reasoning model optimized for multi-step problem solving",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "large context",
      "low latency"
    ],
    "version": "1"
  },
  {
    "capabilities": [
      "streaming"
    ],
    "html_url": "https://github.com/marketplace/models/azureml/Phi-4-multimodal-instruct",
    "id": "microsoft/phi-4-multimodal-instruct",
    "limits": {
      "max_input_tokens": 128000,
      "max_output_tokens": 4096
    },
    "name": "Phi-4-multimodal-instruct",
    "publisher": "Microsoft",
    "rate_limit_tier": "low",
    "registry": "azureml",
    "summary": "First small multimodal model to have 3 modality inputs (text, audio, image), excelling in quality and efficiency",
    "supported_input_modalities": [
      "audio",
      "image",
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "vision",
      "audio",
      "summarization"
    ],
    "version": "2"
  },
  {
    "capabilities": [
      "reasoning",
      "streaming"
    ],
    "html_url": "https://github.com/marketplace/models/azureml/Phi-4-reasoning",
    "id": "microsoft/phi-4-reasoning",
    "limits": {
      "max_input_tokens": 32768,
      "max_output_tokens": 4096
    },
    "name": "Phi-4-Reasoning",
    "publisher": "Microsoft",
    "rate_limit_tier": "low",
    "registry": "azureml",
    "summary": "State-of-the-art open-weight reasoning model.",
    "supported_input_modalities": [
      "text"
    ],
    "supported_output_modalities": [
      "text"
    ],
    "tags": [
      "reasoning",
      "large context",
      "low latency"
    ],
    "version": "1"
  }
]